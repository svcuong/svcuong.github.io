[{"authors":["admin"],"categories":null,"content":"Welcome to my world! Hiện tôi đang là nghiên cứu sinh chuyên ngành phân tích hệ thống, quản lý và xử lý thông tin tại trường Đại học Tổng hợp Kỹ thuật Quốc gia Volgograd (Volgograd Technical State University), Liên Bang Nga. Mục đích chính tôi tạo blog này là để hệ thống lại và chia sẻ kiến thức về khoa học dữ liệu với R và Python. Các bạn có thể theo dõi trang Khoa học dữ liệu với R và Python để nhận thông tin cập nhật các bài viết mới từ blog.\n","date":1591574400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1591574400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/cuong-sai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cuong-sai/","section":"authors","summary":"Welcome to my world! Hiện tôi đang là nghiên cứu sinh chuyên ngành phân tích hệ thống, quản lý và xử lý thông tin tại trường Đại học Tổng hợp Kỹ thuật Quốc gia Volgograd (Volgograd Technical State University), Liên Bang Nga.","tags":null,"title":"Cuong Sai","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb560906b6a99893cc21387348c0b074","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"====\u0026gt; Đang cập nhật\nData Wrangling là gì? Data wrangling là khâu rất quan trọng trong tất cả các dự án về data science: thiếu nó thì bạn không thể làm gì với dữ liệu của bạn. Data Wrangling là thuật ngữ chung cho các khâu sau của phân tích số liệu:\n Data importing: Đọc (nạp ) dữ liệu khác nhau từ các nguồn khác nhau vào môi trường R. Tidy data: tinh chỉnh, sắp xếp dữ liệu có trật tự Data manipulation: Biến đổi dữ liệu  Data Wrangling với R Data Wrangling với Python ","date":1570579200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570579200,"objectID":"19f655ca154e27f26b2b92892136a6b6","permalink":"/courses/data-wrangling/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/courses/data-wrangling/","section":"courses","summary":"Tổng hợp các thao tác làm sạch, tinh chỉnh và biến đổi dữ liệu với R và Python","tags":null,"title":"Data Wrangling với R và Python","type":"docs"},{"authors":null,"categories":null,"content":"====\u0026gt; Đang cập nhật\nData visualization là gì? ===\u0026gt; Đang cập nhật\nData visualization với R ===\u0026gt; Đang cập nhật\nData visualization với Python ===\u0026gt; Đang cập nhật\n","date":1567987200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567987200,"objectID":"881c4f723286372e3eda612793e74662","permalink":"/courses/data-visualization/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/courses/data-visualization/","section":"courses","summary":"Tổng hợp các thao tác trực quan hóa dữ liệu - biểu diễn đồ họa của dữ liệu vơi R và Python","tags":null,"title":"Data visualization với R và Python","type":"docs"},{"authors":null,"categories":null,"content":"====\u0026gt; Đang cập nhật\nLinear Regression K-nearest neighbors Logistic Regression SVM Random forest Xgboost ","date":1570579200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570579200,"objectID":"47c9978cb260487504efcde96a686dd8","permalink":"/courses/machine-learning/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/courses/machine-learning/","section":"courses","summary":"Tổng hợp các Machine Learning algorithms trong R và Python","tags":null,"title":"Machine Learning với R và Python","type":"docs"},{"authors":null,"categories":null,"content":"====\u0026gt; Đang cập nhật\nMLP CNN RNN LSTM Autoencoders ","date":1573257600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1573257600,"objectID":"044540e73f3b2972e5019e70323bddc5","permalink":"/courses/deep-learning/","publishdate":"2019-11-09T00:00:00Z","relpermalink":"/courses/deep-learning/","section":"courses","summary":"Tổng hợp các Deep Learning Networks trong R và Python","tags":null,"title":"Deep Learning với R và Python","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":"R","content":"\rTrong machine learning tồn tại định lý “không có bữa trưa miễn phí” (No free lunch theorem), tức là không tồn tại một thuật toán mà luôn tốt cho mọi ứng dụng và mọi tập dữ liệu, vì các thuật toán machiner learning thường dựa trên một tập các tham số (hyperparameters) hoặc một giả thiết nhất định nào đó về phân bố dữ liệu. Vì vậy để tìm được những thuật toán phù hợp cho tập dataset của mình có thể các bạn sẽ cần nhiều thời gian để test các thuật toán khác nhau. Rồi từ đó thực hiện hiệu chỉnh các tham số (`tuning hyperparameters’) của thuật toán để thu được độ chính xác cao nhất.\nMột cách khác có thể sử dụng để tăng độ chính xác trên tập dataset của bạn là kết hợp (combine) một số mô hình với nhau. Phương pháp này gọi là esemble learning. Ý tưởng của việc combine các mô hình khác nhau xuất phát từ một suy nghĩ hợp lý là: các mô hình khác nhau có khả năng khác nhau, có thể thực hiện tốt nhất các loại công việc khác nhau (subtasks), khi kết hợp các mô hình này với nhau một cách hợp lý thì sẽ tạo thành một mô hình kết hợp (combined model) mạnh có khả năng cải thiện hiệu suât tổng thể (overall performance) so với việc chỉ dùng các mô hình một cách đơn lẻ.\nCác phương pháp Ensemble Learning được chia thành 3 loại sau đây:\n\rBagging (đóng bao)\rBoosting (tăng cường)\rStacking (Xếp chồng)\r\rTrong post này, trước hết tôi sẽ giới thiệu 3 kỹ thuật ensemble learning kể trên, sau đó là cách sử dụng thư viện caret và caretEnsemble trong R để triển khai chúng và áp dụng vào bài toán cụ thể.\nĐể cài đặt 2 thư viện này ta dùng lệnh install.packages(.) với tham số đầu vào là tên thư viện muốn cài:\ninstall.packages(\u0026quot;caret\u0026quot;)\rintall.packages(\u0026quot;caretEnsemble\u0026quot;)\rĐôi nét về thư viện caret: Ngôn ngữ R khác biệt bởi số lượng rất lớn các packages chuyên dụng khác nhau cho phép xây dựng các mô hình dự đoán. Tuy nhiên đây cũng chính là khuyết điểm, khi có quá nhiều các gói triển khai machine learning algorithms dưới dạng các\rhàm rải rác đòi hỏi ta cần nhiều thời gian để tìm kiếm và nắm vững những đặc trưng về cú pháp cũng như cách sử dụng của từng hàm. Để giải quyết vấn đề này Max Kuhn đã xây dựng một giao diện phổ quát cho phép truy cập và sử dụng các machine learning algorithms từ cái gói khác nhau được triển khai trên ngôn ngữ R. Kết quả chính là package caret (viết tắt từ Classification and Regression Training), được công bố đầu tiên vào năm 2008 tại tạp chí phần mềm thống kê Journal of Statistical Software. Gói caret giúp chúng ta tiết kiệm được rất nhiều thời gian trong quá trình phân tích và xây dựng các models. Dưới đây là một số\rđặc trưng cơ bản của gói caret:\n\rSử dụng cú pháp lệnh chung (phổ quát) không phụ thuộc vào cú pháp của các hàm gốc (các hàm triển khai các machine learningalgorithms)\n\rTự động tìm kiếm những giá trị tối ưu cho các hyperparameters của mô hình (tuning parameters)\n\rCó khả năng tổ chức tính toán song song để tăng đáng kể tốc độ quá trình huấn luyện mô hình\n\rSử dụng Caret cho phép giải quyết hầu hết các nhiệm vụ trong machine learning từ tiền xủ lý cho đến đánh giá mô hình\n\r\r1. Phân biệt 3 kỹ thuật boosting, baggig và statcking\nBagging xây dựng một lượng lớn các models (thường là cùng loại) trên những subsamples khác nhau từ tập training dataset một cách song song nhằm đưa ra dự đoán tốt hơn.\nBoosting xây dựng một lượng lớn các models (thường là cùng loại). Tuy nhiên quá trình huấn luyện trong phương pháp này diễn ra tuần tự theo chuỗi (sequence). Trong chuỗi này mỗi model sau sẽ học cách sửa những errors của model trước (hay nói cách khác là dữ liệu mà model trước dự đoán sai).\nNguồn ảnh\n\rStacking xây dựng một số models (thường là khác loại) và một mô hình supervisor model, mô hình này sẽ học cách kết hợp kết quả dự báo của một số mô hình một cách tốt nhất.\nNguồn ảnh\n\r2. Thực hành\nNạp các thư viện cần dùng vào phiên làm việc của R để thực hành:\nlibrary(caret)\rlibrary(caretEnsemble) \rKiểm tra số lượng các machine learning algorithms trong R được hỗ trợ bởi caret:\ncarets \u0026lt;- getModelInfo()\rcarets.names \u0026lt;- names(carets)\rlength(carets.names)\r## [1] 238\r2.1 Dữ liệu để thực hành\nĐể thực hành tôi lựa chọn bài toán phân loại nhị phân (binary classification) với tập dữ liệu ionoshene. Trong bài toán này chúng ta cần dự đoán xem cao tần trả vể từ năng lượng của các hạt trong khí quyển có cấu trúc hay là không. Để tìm hiểu thêm về bài toán này các bạn có thể đọc ở đây.\nLoad dữ liệu từ gói mlbench:\n# Load the dataset\rlibrary(mlbench)\rdata(Ionosphere)\rdataset \u0026lt;- Ionosphere\r2.1.1 Thống kê mô tả (descriptive statistics)\nKiểm tra kích thước tập dữ liệu:\ndim(dataset)\r## [1] 351 35\rKiểm tra cấu trúc của tập dữ liệu:\nstr(dataset)\r## \u0026#39;data.frame\u0026#39;: 351 obs. of 35 variables:\r## $ V1 : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 1 2 2 ...\r## $ V2 : Factor w/ 1 level \u0026quot;0\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ...\r## $ V3 : num 0.995 1 1 1 1 ...\r## $ V4 : num -0.0589 -0.1883 -0.0336 -0.4516 -0.024 ...\r## $ V5 : num 0.852 0.93 1 1 0.941 ...\r## $ V6 : num 0.02306 -0.36156 0.00485 1 0.06531 ...\r## $ V7 : num 0.834 -0.109 1 0.712 0.921 ...\r## $ V8 : num -0.377 -0.936 -0.121 -1 -0.233 ...\r## $ V9 : num 1 1 0.89 0 0.772 ...\r## $ V10 : num 0.0376 -0.0455 0.012 0 -0.164 ...\r## $ V11 : num 0.852 0.509 0.731 0 0.528 ...\r## $ V12 : num -0.1776 -0.6774 0.0535 0 -0.2028 ...\r## $ V13 : num 0.598 0.344 0.854 0 0.564 ...\r## $ V14 : num -0.44945 -0.69707 0.00827 0 -0.00712 ...\r## $ V15 : num 0.605 -0.517 0.546 -1 0.344 ...\r## $ V16 : num -0.38223 -0.97515 0.00299 0.14516 -0.27457 ...\r## $ V17 : num 0.844 0.055 0.838 0.541 0.529 ...\r## $ V18 : num -0.385 -0.622 -0.136 -0.393 -0.218 ...\r## $ V19 : num 0.582 0.331 0.755 -1 0.451 ...\r## $ V20 : num -0.3219 -1 -0.0854 -0.5447 -0.1781 ...\r## $ V21 : num 0.5697 -0.1315 0.7089 -0.6997 0.0598 ...\r## $ V22 : num -0.297 -0.453 -0.275 1 -0.356 ...\r## $ V23 : num 0.3695 -0.1806 0.4339 0 0.0231 ...\r## $ V24 : num -0.474 -0.357 -0.121 0 -0.529 ...\r## $ V25 : num 0.5681 -0.2033 0.5753 1 0.0329 ...\r## $ V26 : num -0.512 -0.266 -0.402 0.907 -0.652 ...\r## $ V27 : num 0.411 -0.205 0.59 0.516 0.133 ...\r## $ V28 : num -0.462 -0.184 -0.221 1 -0.532 ...\r## $ V29 : num 0.2127 -0.1904 0.431 1 0.0243 ...\r## $ V30 : num -0.341 -0.116 -0.174 -0.201 -0.622 ...\r## $ V31 : num 0.4227 -0.1663 0.6044 0.2568 -0.0571 ...\r## $ V32 : num -0.5449 -0.0629 -0.2418 1 -0.5957 ...\r## $ V33 : num 0.1864 -0.1374 0.5605 -0.3238 -0.0461 ...\r## $ V34 : num -0.453 -0.0245 -0.3824 1 -0.657 ...\r## $ Class: Factor w/ 2 levels \u0026quot;bad\u0026quot;,\u0026quot;good\u0026quot;: 2 1 2 1 2 1 2 1 2 1 ...\rHiển thị 5 hàng dữ liệu đầu tiên:\nhead(dataset, 5)\r## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10\r## 1 1 0 0.99539 -0.05889 0.85243 0.02306 0.83398 -0.37708 1.00000 0.03760\r## 2 1 0 1.00000 -0.18829 0.93035 -0.36156 -0.10868 -0.93597 1.00000 -0.04549\r## 3 1 0 1.00000 -0.03365 1.00000 0.00485 1.00000 -0.12062 0.88965 0.01198\r## 4 1 0 1.00000 -0.45161 1.00000 1.00000 0.71216 -1.00000 0.00000 0.00000\r## 5 1 0 1.00000 -0.02401 0.94140 0.06531 0.92106 -0.23255 0.77152 -0.16399\r## V11 V12 V13 V14 V15 V16 V17 V18 V19\r## 1 0.85243 -0.17755 0.59755 -0.44945 0.60536 -0.38223 0.84356 -0.38542 0.58212\r## 2 0.50874 -0.67743 0.34432 -0.69707 -0.51685 -0.97515 0.05499 -0.62237 0.33109\r## 3 0.73082 0.05346 0.85443 0.00827 0.54591 0.00299 0.83775 -0.13644 0.75535\r## 4 0.00000 0.00000 0.00000 0.00000 -1.00000 0.14516 0.54094 -0.39330 -1.00000\r## 5 0.52798 -0.20275 0.56409 -0.00712 0.34395 -0.27457 0.52940 -0.21780 0.45107\r## V20 V21 V22 V23 V24 V25 V26 V27\r## 1 -0.32192 0.56971 -0.29674 0.36946 -0.47357 0.56811 -0.51171 0.41078\r## 2 -1.00000 -0.13151 -0.45300 -0.18056 -0.35734 -0.20332 -0.26569 -0.20468\r## 3 -0.08540 0.70887 -0.27502 0.43385 -0.12062 0.57528 -0.40220 0.58984\r## 4 -0.54467 -0.69975 1.00000 0.00000 0.00000 1.00000 0.90695 0.51613\r## 5 -0.17813 0.05982 -0.35575 0.02309 -0.52879 0.03286 -0.65158 0.13290\r## V28 V29 V30 V31 V32 V33 V34 Class\r## 1 -0.46168 0.21266 -0.34090 0.42267 -0.54487 0.18641 -0.45300 good\r## 2 -0.18401 -0.19040 -0.11593 -0.16626 -0.06288 -0.13738 -0.02447 bad\r## 3 -0.22145 0.43100 -0.17365 0.60436 -0.24180 0.56045 -0.38238 good\r## 4 1.00000 1.00000 -0.20099 0.25682 1.00000 -0.32382 1.00000 bad\r## 5 -0.53206 0.02431 -0.62197 -0.05707 -0.59573 -0.04608 -0.65697 good\rKiểm tra missing values trong dữ liệu:\nsum(is.na(dataset))\r## [1] 0\rKiểm tra phân phối của từng thuộc tính:\nsummary(dataset)\r## V1 V2 V3 V4 V5 ## 0: 38 0:351 Min. :-1.0000 Min. :-1.00000 Min. :-1.0000 ## 1:313 1st Qu.: 0.4721 1st Qu.:-0.06474 1st Qu.: 0.4127 ## Median : 0.8711 Median : 0.01631 Median : 0.8092 ## Mean : 0.6413 Mean : 0.04437 Mean : 0.6011 ## 3rd Qu.: 1.0000 3rd Qu.: 0.19418 3rd Qu.: 1.0000 ## Max. : 1.0000 Max. : 1.00000 Max. : 1.0000 ## V6 V7 V8 V9 ## Min. :-1.0000 Min. :-1.0000 Min. :-1.00000 Min. :-1.00000 ## 1st Qu.:-0.0248 1st Qu.: 0.2113 1st Qu.:-0.05484 1st Qu.: 0.08711 ## Median : 0.0228 Median : 0.7287 Median : 0.01471 Median : 0.68421 ## Mean : 0.1159 Mean : 0.5501 Mean : 0.11936 Mean : 0.51185 ## 3rd Qu.: 0.3347 3rd Qu.: 0.9692 3rd Qu.: 0.44567 3rd Qu.: 0.95324 ## Max. : 1.0000 Max. : 1.0000 Max. : 1.00000 Max. : 1.00000 ## V10 V11 V12 V13 ## Min. :-1.00000 Min. :-1.00000 Min. :-1.00000 Min. :-1.0000 ## 1st Qu.:-0.04807 1st Qu.: 0.02112 1st Qu.:-0.06527 1st Qu.: 0.0000 ## Median : 0.01829 Median : 0.66798 Median : 0.02825 Median : 0.6441 ## Mean : 0.18135 Mean : 0.47618 Mean : 0.15504 Mean : 0.4008 ## 3rd Qu.: 0.53419 3rd Qu.: 0.95790 3rd Qu.: 0.48237 3rd Qu.: 0.9555 ## Max. : 1.00000 Max. : 1.00000 Max. : 1.00000 Max. : 1.0000 ## V14 V15 V16 V17 ## Min. :-1.00000 Min. :-1.0000 Min. :-1.00000 Min. :-1.0000 ## 1st Qu.:-0.07372 1st Qu.: 0.0000 1st Qu.:-0.08170 1st Qu.: 0.0000 ## Median : 0.03027 Median : 0.6019 Median : 0.00000 Median : 0.5909 ## Mean : 0.09341 Mean : 0.3442 Mean : 0.07113 Mean : 0.3819 ## 3rd Qu.: 0.37486 3rd Qu.: 0.9193 3rd Qu.: 0.30897 3rd Qu.: 0.9357 ## Max. : 1.00000 Max. : 1.0000 Max. : 1.00000 Max. : 1.0000 ## V18 V19 V20 V21 ## Min. :-1.000000 Min. :-1.0000 Min. :-1.00000 Min. :-1.0000 ## 1st Qu.:-0.225690 1st Qu.: 0.0000 1st Qu.:-0.23467 1st Qu.: 0.0000 ## Median : 0.000000 Median : 0.5762 Median : 0.00000 Median : 0.4991 ## Mean :-0.003617 Mean : 0.3594 Mean :-0.02402 Mean : 0.3367 ## 3rd Qu.: 0.195285 3rd Qu.: 0.8993 3rd Qu.: 0.13437 3rd Qu.: 0.8949 ## Max. : 1.000000 Max. : 1.0000 Max. : 1.00000 Max. : 1.0000 ## V22 V23 V24 V25 ## Min. :-1.000000 Min. :-1.0000 Min. :-1.00000 Min. :-1.0000 ## 1st Qu.:-0.243870 1st Qu.: 0.0000 1st Qu.:-0.36689 1st Qu.: 0.0000 ## Median : 0.000000 Median : 0.5318 Median : 0.00000 Median : 0.5539 ## Mean : 0.008296 Mean : 0.3625 Mean :-0.05741 Mean : 0.3961 ## 3rd Qu.: 0.188760 3rd Qu.: 0.9112 3rd Qu.: 0.16463 3rd Qu.: 0.9052 ## Max. : 1.000000 Max. : 1.0000 Max. : 1.00000 Max. : 1.0000 ## V26 V27 V28 V29 ## Min. :-1.00000 Min. :-1.0000 Min. :-1.00000 Min. :-1.0000 ## 1st Qu.:-0.33239 1st Qu.: 0.2864 1st Qu.:-0.44316 1st Qu.: 0.0000 ## Median :-0.01505 Median : 0.7082 Median :-0.01769 Median : 0.4966 ## Mean :-0.07119 Mean : 0.5416 Mean :-0.06954 Mean : 0.3784 ## 3rd Qu.: 0.15676 3rd Qu.: 0.9999 3rd Qu.: 0.15354 3rd Qu.: 0.8835 ## Max. : 1.00000 Max. : 1.0000 Max. : 1.00000 Max. : 1.0000 ## V30 V31 V32 V33 ## Min. :-1.00000 Min. :-1.0000 Min. :-1.000000 Min. :-1.0000 ## 1st Qu.:-0.23689 1st Qu.: 0.0000 1st Qu.:-0.242595 1st Qu.: 0.0000 ## Median : 0.00000 Median : 0.4428 Median : 0.000000 Median : 0.4096 ## Mean :-0.02791 Mean : 0.3525 Mean :-0.003794 Mean : 0.3494 ## 3rd Qu.: 0.15407 3rd Qu.: 0.8576 3rd Qu.: 0.200120 3rd Qu.: 0.8138 ## Max. : 1.00000 Max. : 1.0000 Max. : 1.000000 Max. : 1.0000 ## V34 Class ## Min. :-1.00000 bad :126 ## 1st Qu.:-0.16535 good:225 ## Median : 0.00000 ## Mean : 0.01448 ## 3rd Qu.: 0.17166 ## Max. : 1.00000\rThuộc tính thứ V2 chỉ có 1 giá trị là 0 nên có thể loại bỏ:\ndataset$V2 \u0026lt;- NULL\rChuyển thuộc tính V1 từ factor sang numeric:\ndataset$V1 \u0026lt;- as.numeric(as.character(dataset$V1))\rKiểm tra mức độ tương quan (correlation) giữa các thuộc tính (do số lượng thuộc tính lớn nên tôi chỉ hiển thị tương quan giữa 6 thuộc tính đầu làm mẫu):\ncor(dataset[,1:6])\r## V1 V3 V4 V5 V6 V7\r## V1 1.000000000 0.30203392 -0.006528852 0.15615240 0.12760571 0.22186692\r## V3 0.302033923 1.00000000 0.143364804 0.47658695 0.02576751 0.44025437\r## V4 -0.006528852 0.14336480 1.000000000 0.00115185 -0.19030761 -0.05402953\r## V5 0.156152397 0.47658695 0.001151850 1.00000000 0.03832312 0.59707508\r## V6 0.127605707 0.02576751 -0.190307607 0.03832312 1.00000000 -0.01022692\r## V7 0.221866916 0.44025437 -0.054029528 0.59707508 -0.01022692 1.00000000\r2.1.2 Trực quan hóa dữ liệu (data visualization)\nDo số lượng thuộc tính nhiều nên tôi chỉ thực hiện data visualization đối 12 thuộc tính đầu của tập dữ liệu.\nHistogram cho 12 thuộc tính đầu:\npar(mfrow=c(3,4))\rfor(i in 1:12) { hist(dataset[,i], main=names(dataset)[i], breaks = 30)\r}\rBoxplot cho 12 thuộc tính đầu:\nboxplot(dataset[, 1:12], col = \u0026quot;orange\u0026quot;, main = \u0026quot;Features Boxplot\u0026quot;)\rTrong bước này nếu phát hiện trong các thuộc tính có nhiều giá trị ngoại lai (outliers) thì các bạn có thể đọc post trước của tôi về cách loại bỏ outliers trong dữ liệu cho machine learning bằng các phương pháp thống kê tại đây.\n2.1.3 Tiền xử lý dữ liệu (data preprocessing)\nXác định và Loại bỏ các thuộc tính tương quan với nhau cao (\u0026gt;0.75)\n# Tìm các thuộc tính tương quan với nhau cao\rcor_coefficient \u0026lt;- 0.75\rcorrelations \u0026lt;- cor(dataset[,1:13])\rhighlyCorrelated \u0026lt;- findCorrelation(correlations, cutoff=cor_coefficient)\rlength(highlyCorrelated)\r## [1] 0\rỞ đây không có các thuộc tính tương quan cao với nhau, tuy nhiên nếu có thì các bạn có thể loại bỏ chúng như sau:\ndatasetFeatures \u0026lt;- dataset[,-highlyCorrelated]\rdim(datasetFeatures)\rChuẩn hóa giá trị của các thuộc tính (data normalization) về khoảng [0,1]:\npreProcValues \u0026lt;- preProcess(dataset, method = c(\u0026quot;range\u0026quot;))\rdata_processed \u0026lt;- predict(preProcValues, dataset)\rVậy là dữ liệu của chúng ta đã sẵn sàng để test các thuật toán ensemble learning rồi.\n2.2. Thuật toán Boosting\nTrong phạm vi post này tôi sẽ test hai thuật toán boosting khá phổ biến là: C5.0 và Stochastic Gradient Boosting\nDưới đây là ví dụ huấn luyện hai mô hình này trên R với các tham số mặc định:\nseed \u0026lt;- 10\r# tạo một đối tượng control cho cross-validation\rcontrol \u0026lt;- trainControl(method=\u0026quot;repeatedcv\u0026quot;, number=10, repeats=3)\r# Trong đó\r# method = \u0026#39;repeatedcv\u0026#39;: sử dụng cross-validation với các tham số sau:\r# number = 10 có nhĩa là quá trình cross-validation cần chia dữ liệu gốc thành 10 phần bằng nhau\r# repeats = 3 có nhĩa là quá trình cross-validation sẽ hoàn thành sau 3 lần\r# C5.0\rset.seed(seed)\rfit.c50 \u0026lt;- train(Class~., data=dataset, method=\u0026quot;C5.0\u0026quot;, metric = \u0026quot;Accuracy\u0026quot;, trControl=control)\r# Stochastic Gradient Boosting\rset.seed(seed)\rfit.gbm \u0026lt;- train(Class~., data=dataset, method=\u0026quot;gbm\u0026quot;, metric = \u0026quot;Accuracy\u0026quot;, trControl=control, verbose=FALSE)\rSo sánh kết quả hai mô hình:\nboosting_results \u0026lt;- resamples(list(c5.0=fit.c50, gbm=fit.gbm))\rsummary(boosting_results)\r## ## Call:\r## summary.resamples(object = boosting_results)\r## ## Models: c5.0, gbm ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## c5.0 0.8823529 0.9148810 0.9575163 0.9468627 0.9714286 1 0\r## gbm 0.8529412 0.9166667 0.9428571 0.9420184 0.9714286 1 0\r## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## c5.0 0.7213115 0.8157164 0.9069808 0.8806722 0.937833 1 0\r## gbm 0.6586345 0.8142060 0.8776224 0.8707906 0.937201 1 0\rdotplot(boosting_results)\rTừ kết quả so sánh ta thấy thuật toán C5.0 cho kết quả chính xác hơn so với Stochastic Gradient Boosting trong bài toán này (với độ chính xác là 94.68%)\n2.3 Thuật toán Bagging\nChúng ta cùng test hai thuật toán thuộc kỹ thuật Bagging là: Bagged CART và Random Forest\nDưới đây là ví dụ huấn luyện hai mô hình này trên R với các tham số mặc định:\ncontrol \u0026lt;- trainControl(method=\u0026quot;repeatedcv\u0026quot;, number=10, repeats=3)\r# Bagged CART\rset.seed(seed)\rfit.treebag \u0026lt;- train(Class~., data=dataset, method=\u0026quot;treebag\u0026quot;, metric = \u0026quot;Accuracy\u0026quot;, trControl=control)\r# Random Forest\rset.seed(seed)\rfit.rf \u0026lt;- train(Class~., data=dataset, method=\u0026quot;rf\u0026quot;, metric = \u0026quot;Accuracy\u0026quot;, trControl=control)\rSo sánh kết quả hai mô hình:\nbagging_results \u0026lt;- resamples(list(treebag=fit.treebag, rf=fit.rf))\rsummary(bagging_results)\r## ## Call:\r## summary.resamples(object = bagging_results)\r## ## Models: treebag, rf ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## treebag 0.8285714 0.8922269 0.9428571 0.9210566 0.9440476 0.9722222 0\r## rf 0.8235294 0.9142857 0.9428571 0.9343946 0.9714286 1.0000000 0\r## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## treebag 0.6209386 0.7708291 0.8731884 0.8266350 0.8770749 0.9407895 0\r## rf 0.5984252 0.8149436 0.8734173 0.8550575 0.9372010 1.0000000 0\rdotplot(bagging_results)\rTừ kết quả so sánh ta thấy thuật toán Random Forest cho kết quả chính xác hơn so với CART trong bài toán này (với độ chính xác là 93.44%). Tuy nhiên cả hai thuật toán Bagging đều có độ chính xác nhỏ hơn so với 2 thuật toán Boosting trước.\n2.4. Thuật toán Stacking\nĐể kết hợp các mô hình machine learning khác nhau trong R chúng ta sử dụng thư viện caretEnsemble. Với danh sách các caret models, hàm caretStack() của gói này có thể sự dụng để chỉ định mô hình bậc cao hơn, từ đó sẽ học cách tìm sự kết hợp tốt nhất những sub-models với nhau.\nỞ ví dụ này, tôi sẽ sử dụng 5 sub-models sau cho tập dữ liệu ionosphere:\n\rLinear Discriminate Analysis (LDA)\n\rClassification and Regression Trees (CART)\n\rLogistic Regression (GLM)\n\rk-Nearest Neighbors (kNN)\n\rSupport Vector Machine with a Radial Basis Kernel Function (SVM)\n\r\rDưới đây là ví dụ huấn luyện 5 mô hình này trên R với các tham số mặc định:\ncontrol \u0026lt;- trainControl(method=\u0026quot;repeatedcv\u0026quot;, number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)\ralgorithmList \u0026lt;- c(\u0026#39;lda\u0026#39;, \u0026#39;rpart\u0026#39;, \u0026#39;glm\u0026#39;, \u0026#39;knn\u0026#39;, \u0026#39;svmRadial\u0026#39;)\rset.seed(seed)\rmodels \u0026lt;- caretList(Class~., data=dataset, trControl=control, methodList=algorithmList)\rSo sánh kết quả các mô hình:\nresults \u0026lt;- resamples(models)\rsummary(results)\r## ## Call:\r## summary.resamples(object = results)\r## ## Models: lda, rpart, glm, knn, svmRadial ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## lda 0.7941176 0.8297619 0.8571429 0.8669546 0.9136555 0.9428571 0\r## rpart 0.8000000 0.8529412 0.8611111 0.8736819 0.9079365 0.9714286 0\r## glm 0.7428571 0.8539916 0.8823529 0.8824214 0.9166667 0.9714286 0\r## knn 0.7500000 0.8235294 0.8333333 0.8403097 0.8601190 0.9444444 0\r## svmRadial 0.8888889 0.9142857 0.9436508 0.9477591 0.9714286 1.0000000 0\r## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s\r## lda 0.4803493 0.6048824 0.6697323 0.6868903 0.8032314 0.8679245 0\r## rpart 0.5648313 0.6586345 0.7024010 0.7193438 0.7900135 0.9397590 0\r## glm 0.4578313 0.6618591 0.7267975 0.7371380 0.8163265 0.9368030 0\r## knn 0.4087591 0.5641026 0.6196004 0.6199654 0.6770575 0.8754325 0\r## svmRadial 0.7419355 0.8142060 0.8776224 0.8847121 0.9375755 1.0000000 0\rdotplot(results)\rTa thấy trong các mô hình này thì SVM cho kết quả chính xác nhất (94.78%).\nGiờ chúng ta hãy thử dùng kỹ thuật stacking để xem có thể cải thiện được độ chính xác không.\nLưu ý: Khi các bạn muốn kết hợp các mô hình với nhau sử dụng kỹ thuật stacking, thì các bạn cần kiểm chứng rằng kết quả dự báo từ các mô hình này tương quan với nhau thấp. Nếu kết quả dự báo của các sub-models tương quan cao với nhau (\u0026gt; 0.75) thì có nghĩa là chúng sẽ cho kết quả dự báo tương tự nhau, điều này sẽ làm giảm hiệu quả khi ta kết hợp các mô hình này với nhau.\nKiểm tra độ tương quan giữa các sub-models:\nmodelCor(results)\r## lda rpart glm knn svmRadial\r## lda 1.0000000 0.379461533 0.277037721 0.4898435 0.3056838\r## rpart 0.3794615 1.000000000 0.001889458 0.4040556 0.2539580\r## glm 0.2770377 0.001889458 1.000000000 0.1466240 0.4296011\r## knn 0.4898435 0.404055597 0.146623958 1.0000000 0.5495574\r## svmRadial 0.3056838 0.253957967 0.429601141 0.5495574 1.0000000\rsplom(results)\rNhìn vào kết quả ta có thể thấy các su-models cho kết quả dự báo tương quan với nhau thấp theo từng cặp. Cặp tương quan với nhau nhất là SVM và kNN với độ tương quan 0.549, cũng vẫn nhỏ hơn mức quy địn là cao (\u0026gt;0.75).\nNào chúng ta hãy thử kết hợp predictions của các sub-models sử dụng mô hình gml:\nstackControl \u0026lt;- trainControl(method=\u0026quot;repeatedcv\u0026quot;, number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)\rset.seed(seed)\rstack.glm \u0026lt;- caretStack(models, method=\u0026quot;glm\u0026quot;, metric=\u0026quot;Accuracy\u0026quot;, trControl=stackControl)\rprint(stack.glm)\r## A glm ensemble of 5 base models: lda, rpart, glm, knn, svmRadial\r## ## Ensemble results:\r## Generalized Linear Model ## ## 1053 samples\r## 5 predictor\r## 2 classes: \u0026#39;bad\u0026#39;, \u0026#39;good\u0026#39; ## ## No pre-processing\r## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 947, 947, 947, 948, 947, 949, ... ## Resampling results:\r## ## Accuracy Kappa ## 0.9544285 0.9003902\rĐộ chính xác cải thiện lên 95.44% so với chỉ sử dụng SVM model là 94.78%, tuy nhiên cũng chưa có độ chênh lệnh nhiều.\nTiếp theo tôi thử thử kết hợp predictions của các sub-models sử dụng mô hình random forest:\nset.seed(seed)\rstack.rf \u0026lt;- caretStack(models, method=\u0026quot;rf\u0026quot;, metric=\u0026quot;Accuracy\u0026quot;, trControl=stackControl)\rprint(stack.rf)\r## A rf ensemble of 5 base models: lda, rpart, glm, knn, svmRadial\r## ## Ensemble results:\r## Random Forest ## ## 1053 samples\r## 5 predictor\r## 2 classes: \u0026#39;bad\u0026#39;, \u0026#39;good\u0026#39; ## ## No pre-processing\r## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 947, 947, 947, 948, 947, 949, ... ## Resampling results across tuning parameters:\r## ## mtry Accuracy Kappa ## 2 0.9623381 0.9177343\r## 3 0.9588700 0.9103978\r## 5 0.9569833 0.9064705\r## ## Accuracy was used to select the optimal model using the largest value.\r## The final value used for the model was mtry = 2.\rĐộ chính xác cũng cải thiện hơn so với chỉ dùng svm model (96.23%).\nTham khảo:\rHow to Build an Ensemble Of Machine Learning Algorithms in R\n","date":1598140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598193946,"objectID":"535958c2e3329814e5296d5b0751ab2e","permalink":"/post/ensemble-learning/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/post/ensemble-learning/","section":"post","summary":"Trong machine learning tồn tại định lý “không có bữa trưa miễn phí” (No free lunch theorem), tức là không tồn tại một thuật toán mà luôn tốt cho mọi ứng dụng và mọi tập dữ liệu, vì các thuật toán machiner learning thường dựa trên một tập các tham số (hyperparameters) hoặc một giả thiết nhất định nào đó về phân bố dữ liệu.","tags":["Data Science","Data Visualization"],"title":"Phương pháp Ensemble Learning trong Machine Learning: Boosting, Bagging, Stacking (Sử dụng R code)","type":"post"},{"authors":[],"categories":"R","content":"\rWeb scraping là công việc trích xuất dữ liệu từ một trang web. Trong thực tế đôi khi bạn cần thu thập 1 số lượng lớn thông tin từ một số trang web như là giá cổ phiếu, chi tiết sản phẩm, số liệu thống kê thể thao, thông tin liên hệ của công ty,…để phục vụ cho nhiều mục đích khác nhau. Tuy nhiên, việc lấy các thông tin này một cách thủ công thì cần rất nhiều thời gian. Lúc này web scraping chính là cách giúp chúng ta trích xuất dữ liệu từ các trang web một cách tự động. Kỹ thuật này chủ yếu tập trung vào việc chuyển đổi dữ liệu phi cấu trúc (HTML) trên web thành dữ liệu có cấu trúc (cơ sở dữ liệu, bảng tính,…)\nCoinMarketCap là một trang web chuyên cung cấp dữ liệu về các loại tiền điện tử đang được lưu hành trên thế giới. Nếu bạn là người đã hoặc đang tìm hiểu, đầu tư vào các đồng coins thì không thể nào bỏ qua trang web cung cấp thông tin về tiền điện tử lớn nhất thế giới này. Với CoinMarketCap bạn có thể xem rất nhiều lọai thông tin ví dụ như: biến động giá cả các đồng coin, trữ lượng, khối lượng giao dịch theo từng khoảng thời gian, tỷ giá, xem trang web chính thức hoặc các hoạt động trên mạng xã hội của các đồng coins.\nTrong bài này tôi sẽ giới thiệu với các bạn cách sử dụng gói rvest trong R để scrape dữ liệu lịch sử thị trường các loại tiền điện tử từ CoinMarketCap. Sau đó dùng gói ggplot2 để trực quan hóa dữ liệu nhằm mục đích so sánh biến động giá cả của các đồng coins với nhau trong khoảng thời gian nhất định.\nGói rvest cũng giống như thư viện BeautifulSoup trên Python, là một thư viện R để trích xuất dữ liệu từ các tập tin HTML.\nLưu ý khi scape một trang web:\n\rKhông nên tạo quá nhiều request đến trang web trong 1 thời điểm, vì nó có thể gây ra sập trang web (có thể bị liệt vào tấn công từ chối dịch vụ DDoS)\rTrang web có thể sẽ thường xuyên thay đổi giao diện, bố cục (HTML) nên bạn cần thường xuyên cập nhật code của mình\r\rNạp các thư viện cần dùng vào phiên làm việc của R để thực hành:\nlibrary(jsonlite) # Thư viện cho phép đọc json files\rlibrary(rvest) # thư viện để scape web\rlibrary(viridis) # Thay đổi bảng màu mặc định ggplot2\rlibrary(tidyverse) # Hệ sinh thái các gói hỗ trợ biến đổi (dplyr) và trực quan hóa dữ liệu (ggplot2)\rNếu chưa cài các thư viện trên thì các bạn dùng lệnh install.packages(.) để cài đặt với tham số đầu vào là tên các thư viện cần cài đặt. Ví dụ:\ninstall.packages(\u0026quot;rvest\u0026quot;)\rĐầu tiên chúng ta đọc json file từ trang CoinMarketCap để biết thông tin về các loại coins:\njson \u0026lt;- \u0026quot;https://s2.coinmarketcap.com/generated/search/quick_search.json\u0026quot;\rcoins \u0026lt;- read_json(json, simplifyVector = TRUE)\rHiển thị bảng thông tin 10 đồng coins đầu tiên theo rank:\nhead(coins,10)\r## name symbol rank slug\r## 1 Bitcoin BTC 1 bitcoin\r## 2 Ethereum ETH 2 ethereum\r## 3 XRP XRP 3 xrp\r## 4 Tether USDT 4 tether\r## 5 Bitcoin Cash BCH 5 bitcoin-cash\r## 6 Chainlink LINK 6 chainlink\r## 7 Litecoin LTC 7 litecoin\r## 8 Bitcoin SV BSV 8 bitcoin-sv\r## 9 Cardano ADA 9 cardano\r## 10 Crypto.com Coin CRO 10 crypto-com-coin\r## tokens id\r## 1 Bitcoin, bitcoin, BTC 1\r## 2 Ethereum, ethereum, ETH 1027\r## 3 XRP, xrp, XRP 52\r## 4 Tether, tether, USDT 825\r## 5 Bitcoin Cash, bitcoin-cash, BCH 1831\r## 6 Chainlink, chainlink, LINK 1975\r## 7 Litecoin, litecoin, LTC 2\r## 8 Bitcoin SV, bitcoin-sv, BSV 3602\r## 9 Cardano, cardano, ADA 2010\r## 10 Crypto.com Coin, crypto-com-coin, CRO 3635\rKiểm tra tổng số lượng đồng tiền kỹ thuật số đang lưu hành trên CoinMarketCap:\ndim(coins)[1]\r## [1] 4812\rĐể ví dụ, trong bài này tôi sẽ so sánh thông tin 10 đồng coins đầu tiên có rank từ 2 đến 11:\n# Lấy thông tin về tên, symbol và slug của 10 đồng coins có rank = [2:11]\rcoinslug \u0026lt;- coins$slug[2:11]\rcoinname \u0026lt;- coins$name[2:11]\rcoinsymbol \u0026lt;- coins$symbol[2:11]\rTạo hàm để get dữ liệu coins. Khi ta vào mục historical data (như hình bên dưới) để xem thông tin của từng loại coin trên trang CoinMarketCap, ta thấy trong đường link có 3 thông tin quan trọng đó là:\n\rslug: Slug của loại coin đó (như đường link trong hình dưới là bitcoin)\rstart: Ngày bắt đầu của khoảng thời gian ta muốn xem thông tin\rend: ngày kết thúc của khoảng thời gian ta muốn xem thông tin\r\rScreenshot từ trang CoinMarketCap\n\rVì vậy ta sẽ viết hàm để get dữ liệu dựa vào 3 thông tin trên như sau:\n# Tạo hàm get dữ liệu coin\rget_data_coin \u0026lt;- function(coin, start_date, end_date){\r# coin: slug của coin\r# start_date: Ngày bắt đầu lấy dữ liệu\r# end_date: Ngày kết thúc lấy dữ liệu\r# Tạo đường link từ 3 thông tin đầu vào\rhistoryurl \u0026lt;- paste0(\u0026quot;https://coinmarketcap.com/currencies/\u0026quot;,\rcoin,\r\u0026quot;/historical-data/?start=\u0026quot;,\rstart_date,\r\u0026quot;\u0026amp;end=\u0026quot;,\rend_date)\r# Đọc dữ liệu từ web\rurl \u0026lt;-read_html(historyurl)\rdata \u0026lt;- data.frame(html_table(url)[[3]])\rdata$Slug \u0026lt;- coin\rreturn(data)\r}\rVí dụ sử dụng hàm get_data_coin() vừa tạo để lấy dữ liệu lịch sử của 10 loại coins trên từ ngày 21/08/2019 dến ngày 21/08/2020:\n# Nhập ngày bắt đầu và và kết thúc trong khoảng thời gian muốn xem theo định đạng \u0026quot;Năm-Tháng-Ngày\u0026quot;\rstart_date = 20190821\rend_date = 20200821\r# Tạo list trống để lưu dữ liệu coins\rresults_data \u0026lt;- list()\r# Lưu dữ liệu từng loại coin trong danh sách 10 coins ở trên vào list trống\rfor (i in (1:length(coinslug))){\rresults_data[[i]] \u0026lt;- get_data_coin(coin = coinslug[i], start_date = start_date, end_date = end_date)\r}\r# Gộp data.frame của từng loại coin trong list vào thành 1 bảng\rresults_data \u0026lt;- do.call(rbind, results_data)\rĐể kiểm tra dữ liệu thu được, ta hiển thị 10 hàng dữ liệu đầu tiên trong bẳng bằng lệnh head():\nhead(results_data, 10)\r## Date Open. High Low Close.. Volume Market.Cap\r## 1 Aug 21, 2020 416.15 418.64 387.44 389.13 11,781,796,374 43,690,315,907\r## 2 Aug 20, 2020 406.76 418.73 404.03 416.44 10,043,032,427 46,751,356,941\r## 3 Aug 19, 2020 423.74 427.02 396.68 406.46 13,137,391,167 45,625,864,918\r## 4 Aug 18, 2020 429.67 432.58 419.67 423.67 11,978,984,079 47,551,407,033\r## 5 Aug 17, 2020 433.97 442.73 422.65 429.53 13,227,089,410 48,203,426,751\r## 6 Aug 16, 2020 433.35 436.27 415.09 433.79 12,168,816,874 48,675,162,267\r## 7 Aug 15, 2020 437.56 441.75 429.87 433.35 12,416,067,894 48,620,820,125\r## 8 Aug 14, 2020 428.68 444.58 423.35 437.40 15,064,589,987 49,068,474,083\r## 9 Aug 13, 2020 390.84 432.90 379.71 428.74 18,480,303,526 48,091,569,052\r## 10 Aug 12, 2020 380.06 391.31 367.92 391.02 12,408,772,745 43,855,501,072\r## Slug\r## 1 ethereum\r## 2 ethereum\r## 3 ethereum\r## 4 ethereum\r## 5 ethereum\r## 6 ethereum\r## 7 ethereum\r## 8 ethereum\r## 9 ethereum\r## 10 ethereum\rThêm thông tin gồm name và symbol của từng coin vào bảng dữ liệu:\ncoinnames \u0026lt;- data.frame(\rName = as.character(coinname),\rSymbol = as.character(coinsymbol),\rSlug = as.character(coinslug))\rdf \u0026lt;- full_join(coinnames, results_data, by = \u0026quot;Slug\u0026quot;)\rĐể kiểm tra bảng dữ liệu mới thu được, ta hiển thị 10 hàng dữ liệu đầu tiên trong bảng:\nhead(df, 10)\r## Name Symbol Slug Date Open. High Low Close..\r## 1 Ethereum ETH ethereum Aug 21, 2020 416.15 418.64 387.44 389.13\r## 2 Ethereum ETH ethereum Aug 20, 2020 406.76 418.73 404.03 416.44\r## 3 Ethereum ETH ethereum Aug 19, 2020 423.74 427.02 396.68 406.46\r## 4 Ethereum ETH ethereum Aug 18, 2020 429.67 432.58 419.67 423.67\r## 5 Ethereum ETH ethereum Aug 17, 2020 433.97 442.73 422.65 429.53\r## 6 Ethereum ETH ethereum Aug 16, 2020 433.35 436.27 415.09 433.79\r## 7 Ethereum ETH ethereum Aug 15, 2020 437.56 441.75 429.87 433.35\r## 8 Ethereum ETH ethereum Aug 14, 2020 428.68 444.58 423.35 437.40\r## 9 Ethereum ETH ethereum Aug 13, 2020 390.84 432.90 379.71 428.74\r## 10 Ethereum ETH ethereum Aug 12, 2020 380.06 391.31 367.92 391.02\r## Volume Market.Cap\r## 1 11,781,796,374 43,690,315,907\r## 2 10,043,032,427 46,751,356,941\r## 3 13,137,391,167 45,625,864,918\r## 4 11,978,984,079 47,551,407,033\r## 5 13,227,089,410 48,203,426,751\r## 6 12,168,816,874 48,675,162,267\r## 7 12,416,067,894 48,620,820,125\r## 8 15,064,589,987 49,068,474,083\r## 9 18,480,303,526 48,091,569,052\r## 10 12,408,772,745 43,855,501,072\rTa thấy cột Date ở dạng character nên cần chuyển về dạng date như sau:\ndf$Date \u0026lt;- lubridate::mdy(unlist(df$Date))\rhead(df, 10)\r## Name Symbol Slug Date Open. High Low Close..\r## 1 Ethereum ETH ethereum 2020-08-21 416.15 418.64 387.44 389.13\r## 2 Ethereum ETH ethereum 2020-08-20 406.76 418.73 404.03 416.44\r## 3 Ethereum ETH ethereum 2020-08-19 423.74 427.02 396.68 406.46\r## 4 Ethereum ETH ethereum 2020-08-18 429.67 432.58 419.67 423.67\r## 5 Ethereum ETH ethereum 2020-08-17 433.97 442.73 422.65 429.53\r## 6 Ethereum ETH ethereum 2020-08-16 433.35 436.27 415.09 433.79\r## 7 Ethereum ETH ethereum 2020-08-15 437.56 441.75 429.87 433.35\r## 8 Ethereum ETH ethereum 2020-08-14 428.68 444.58 423.35 437.40\r## 9 Ethereum ETH ethereum 2020-08-13 390.84 432.90 379.71 428.74\r## 10 Ethereum ETH ethereum 2020-08-12 380.06 391.31 367.92 391.02\r## Volume Market.Cap\r## 1 11,781,796,374 43,690,315,907\r## 2 10,043,032,427 46,751,356,941\r## 3 13,137,391,167 45,625,864,918\r## 4 11,978,984,079 47,551,407,033\r## 5 13,227,089,410 48,203,426,751\r## 6 12,168,816,874 48,675,162,267\r## 7 12,416,067,894 48,620,820,125\r## 8 15,064,589,987 49,068,474,083\r## 9 18,480,303,526 48,091,569,052\r## 10 12,408,772,745 43,855,501,072\rDùng thư viện ggplot2 để trực quan hóa và so sánh biến động giá cả của các loại coins trong khoảng thời gian từ ngày 21/08/2019 dến ngày 21/08/2020:\n# Chỉ so sánh thông tìn các cột Open, High, Low, Close\rdf \u0026lt;- df[,1:8]\r# Chuyển bẳng dữ liệu từ dạng bảng rộng về dài\rlibrary(tidyr)\rdf$Name\u0026lt;- factor(df$Name)\rdf_tall \u0026lt;- df%\u0026gt;% gather(key = s, value = value, -Name, -Symbol, -Slug, -Date)\rdf_tall$s \u0026lt;- factor(df_tall$s, levels=unique(as.character(df_tall$s)))\r# Vẽ biểu đồ so sánh thông tin\rggplot(df_tall, aes(x = Date, y = value, color = Name)) +\rgeom_line(size = 0.75) +\rxlab(\u0026quot;Time\u0026quot;)+\rfacet_wrap(~s, ncol = 2, scales = \u0026quot;free_y\u0026quot;)+\rtheme_bw()\rKhi đã có dữ liệu rồi tiếp theo chúng ta có thể xây dựng các mô hình dự báo để đự đoán giá các loại coins\nChú thích:\n\rOpen: Giá mở cửa\rClose: Giá đóng cửa\rHigh: Mức giá cao nhất\rLow: Mức giá thấp nhất\r\r4 bộ phận này cấu thành nên một nến giao dịch đại diện cho sự lên xuống của giá trị một loại tài sản trong một khoảng thời gian cố định. Điểm “open” của nến đại diện cho mức giá của loại tài sản khi bắt đầu khoảng thời gian giao dịch, trong khi “close” là mức giá khi kết thúc thời gian giao dịch. Còn “high” và “low” lần lượt là các mức giá cao nhất và thấp nhất đạt được trong phiên giao dịch.\nCác bộ phận của một nến giao dịch, với nến tăng (màu xanh) và nếu giảm (màu đỏ)\n\r","date":1598054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598107546,"objectID":"08be2c5524f384118b95cea0cef5e187","permalink":"/post/crypto/","publishdate":"2020-08-22T00:00:00Z","relpermalink":"/post/crypto/","section":"post","summary":"Web scraping là công việc trích xuất dữ liệu từ một trang web. Trong thực tế đôi khi bạn cần thu thập 1 số lượng lớn thông tin từ một số trang web như là giá cổ phiếu, chi tiết sản phẩm, số liệu thống kê thể thao, thông tin liên hệ của công ty,…để phục vụ cho nhiều mục đích khác nhau.","tags":["Data Science","Data Visualization"],"title":"Cách scrape một trang web bằng R. Scrape và so sánh dữ liệu lịch sử thị trường  tiền điện tử từ CoinMarketCap","type":"post"},{"authors":[],"categories":["R","Python","Statistics"],"content":" Nguồn ảnh Outliers (dữ liệu ngoại lai hay là nhiễu) là một trong những thuật ngữ được sử dụng rất rộng rãi trong thế giới data science. Trong quá trình xây dựng các mô hình dự đoán, việc xác định và loại bỏ outliers trong dữ liệu là một bước vô cùng quan trọng. Nó giúp tăng cao độ chính xác cho các mô hình dự đoán.\nKhi phân tích, chúng ta thường dùng các tham số như là mean, median và mode để biết xu hướng tập trung của dữ liệu. Tuy nhiên, một câu hỏi quan trọng cần phải trả lời khi xem xét chất lượng của một mẫu dữ liệu trong phân tích đó là \u0026ldquo;làm sao để đo được độ biến động (hay độ phân tán) của mẫu dữ liệu đó\u0026rdquo;?. Vì chúng ta có thể có 2 mẫu dữ liệu với cùng giá trị mean nhưng độ biến động của chúng lại hoàn toàn khác nhau. Trong thống kê những đại lượng phổ biến nhất để đo lường tiêu chí này là khoảng phần tư (interquartile range, IQR) (hay còn được gọi là khoảng cách giữa các tứ phân vị), phương sai (variance) và độ lệch chuẩn (standard deviation, STD).\nỞ post này tôi sẽ giới thiệu với các bạn cách sử dụng 2 phương pháp thống kê trong R và Python để xác định và loại bỏ outliers trong dữ liệu đó là:\n STD có thể sử dụng để xác định outliers trong dữ liệu có dạng/gần như dạng phân phối chuẩn (hay còn gọi là phân phối Gauss) IQR có thể sử dụng để xác định và loại bỏ outliers không phụ thuộc vào dạng phân phối của dữ liệu.  Và ở cuối post tôi sẽ hướng dẫn các bạn viết hàm tự động xác định và loại bỏ outliers từ dữ liệu sử dụng hai phương pháp trên.\n 1. Tạo dữ liệu để thực hành\nĐể thực hành tôi sử dụng hàm mô phỏng phân phối chuẩn rnorm() trong R để tạo ra dãy số ngẫu nhiên gồm 5000 số với các tham số giá trị trung bình là 20 và độ lệnh chuẩn là 2 như sau:\n# R\rdata = rnorm(5000, mean = 20, sd = 2)\r Với Python thì ta thực hiện như sau:\nTrước hết cần nạp thư viện reticulate để sử dụng Python trong R:\n# R\rlibrary(reticulate)\r Cụ thể về cách sử dụng thư viện reticulate để kết hợp R và Python tôi đã giới thiệu ở post trước, các bạn có thể đọc ở đây\nTạo dữ liệu trong python:\n# Python\r# Tạo dữ liệu tương tự như trong R\rfrom numpy.random import randn\rdata = 2* randn(5000) + 20\r Trong dữ liệu được tạo ra từ phân phối chuẩn sẽ có một số giá trị nằm cách xa giá trị trung bình mean mà chúng ta có thể xác định là outliers.\nBiểu diễn dữ liệu bằng histogam sử dụng hàm hist():\n# R\rhist(data, breaks= 60, main=\u0026quot;Histogram With breaks=60\u0026quot;)\r Nhân tiện đây tôi cũng xin giới thiệu một số hàm hỗ trợ cho các tính toán thống kê trong R như: summary(), sample(), dnorm(), pnorm(), qnorm(), dunif(), punif(), qunif(), runif(), mean(), sd(), cov(), cor(),\u0026hellip;\nHàm summay() cho phép thực hiện thống kê mô tả (descriptive statistics) để cung cấp cho chúng ta một số thông tin thống kê cơ bản về một biến số:\n# R\rsummary(data)\r ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.36 18.75 20.09 20.06 21.43 27.94\r Ví dụ sử dụng hàm sample() để tạo mẫu ngẫu nhiên có lặp lại 10 số nguyên từ 0 đến 9:\n# R\rsample(0:9, replace = TRUE)\r ## [1] 2 5 4 8 9 4 6 5 1 2\r Ví dụ sử dụng hàm tính mật độ phân phối chuẩn dnorm() để ước tính xác xuất của học sinh có điểm là 16.5 biết rằng điểm của học sinh tuân theo phân phối chuẩn với giá trị trung bình là 15, độ lệnh chuẩn là 2.5:\n# R\rdnorm(16.5, mean = 15, sd = 2.5)\r ## [1] 0.1332898\r Tiếp theo ví dụ trên để ước tính xác suất học sinh có điểm tối thiểu là 16.5, ta có thể sử dụng hàm tính xác suất chuẩn tích lũy pnorm() như sau:\n# R\r1 - pnorm(16.5, mean = 15, sd = 2.5)\r ## [1] 0.2742531\r Chức năng của các hàm R còn lại cũng như các hàm tương tự trong Python các bạn có thể tự tìm hiểu thêm.\n2. Phương pháp STD\nNếu như biết được rằng dữ liệu có dạng phân phối Gauss thì chúng ta có thể sử dụng STD trong vài trò là thước đo giới hạn độ phân tán của dữ liệu để xác định outliers.\nTrong phân phối Gauss dựa vào giá trị trung bình mean và STD cho phép chúng ta kiểm tra được độ phân tán (hay là phần trăm bao phủ) của dữ liệu đó như thế nào. Ví dụ:\n Độ bao phủ với 1 STD từ mean là 68% Độ bao phủ với 2 STD từ mean là 95% Độ bao phủ với 3 STD từ mean là 99.7%  Nguồn ảnh\nVậy nên với dữ liệu dạng Gauss có độ phân tán bình thường thì với 3STD, chúng ta sẽ bao phủ được khoảng \u0026gt; 99% của dữ liệu. Từ đó những điểm dữ liệu nằm ngoài 3STD sẽ được coi là outliers.\nCác bước xác định outliers bằng phương pháp STD như sau:\nBước 1: Tính mean và std\n# tính mean và std\r# R\rmean_data \u0026lt;- mean(data)\rstd_data \u0026lt;- sd(data)\r # Python\rfrom numpy import mean\rfrom numpy import std\rmean_data, std_data = mean(data), std(data)\r Bước 2: Tính giá trị biên Upper/Lower để xác định outliers\n# thiết lập giới hạn để xác định outliers\r# R\rlimit_std = 3*std_data\rlower_std = mean_data - limit_std\rupper_std = mean_data + limit_std\r # Python\rlimit_std = 3*std_data\rlower_std, upper_std = mean_data - limit_std, mean_data + limit_std\r Bước 3: Xác định và loại bỏ outliers dựa trên giá trị biên\n# xác định outliers\r# R\rouliers_index_std \u0026lt;- which(data \u0026gt; upper_std | data \u0026lt; lower_std)\rprint(paste(\u0026quot;Number of outliers:\u0026quot;, length(ouliers_index_std)))\r ## [1] \u0026quot;Number of outliers: 18\u0026quot;\r # Python\rouliers_index_std = [x for x in data if x \u0026lt; lower_std or x \u0026gt; upper_std]\rprint('Number of outliers: %d' % len(ouliers_index_std))\r ## Number of outliers: 14\r # Loại bỏ outliers\r# R\rdata_new_std \u0026lt;- data[-ouliers_index_std]\rprint(paste(\u0026quot;Number of Non-outliers:\u0026quot;, length(data_new_std)))\r ## [1] \u0026quot;Number of Non-outliers: 4982\u0026quot;\r # Python\rdata_new_std = [x for x in data if x \u0026gt;= lower_std and x \u0026lt;= upper_std]\rprint('Number of Non-outliers:: %d' % len(data_new_std))\r ## Number of Non-outliers:: 4986\r 2. Phương pháp IQR\nTứ phân vị là đại lượng mô tả sự phân bố và sự phân tán của tập dữ liệu. Tứ phân vị có 3 giá trị, đó là tứ phân vị thứ nhất Q1 (25th), thứ hai Q2 (50th) hay median, và thứ ba Q3 (75th). Ba giá trị này chia một tập hợp dữ liệu (đã sắp xếp dữ liệu theo trật từ từ bé đến lớn) thành 4 phần có số lượng quan sát đều nhau. Tứ phân vị được xác định như sau:\n Sắp xếp các số theo thứ tự tăng dần Cắt dãy số thành 4 phàn bằng nhau Tứ phân vị là các giá trị tại vị trí cắt  Nguồn ảnh\nIQR là sự khác biệt giữa tứ phân vị thứ nhất Q1 và tứ phân vị thứ ba Q3:\n$$IQR = Q_3 - Q_1$$\nGiá trị IQR có thể sử dụng để xác định outliers bằng cách thiết lập các giá trị biên Upper/Lower giống với phương pháp STD như sau: Nếu chúng ta trừ đi kxIQR từ tứ phân vị đầu tiên Q1, bất kỳ giá trị dữ liệu nào nhỏ hơn con số này được coi là giá trị outliers. Tương tự như vậy, nếu chúng ta thêm kxIQR đến tứ phân vị thứ ba Q3, bất kỳ giá trị dữ liệu nào lớn hơn con số này được coi là outliers. Giá trị k thường được chọn là 1.5. Trong trường hợp xác định các extreme outliers có thể dùng giá trị k = 3.\nCác bước xác định outliers bằng phương pháp IQR:\nBước 1: Tính IQR\n# Tính IQR\r# R\rq25 \u0026lt;- quantile(data, 0.25)\rq75 \u0026lt;- quantile(data, 0.75)\riqr \u0026lt;- q75 - q25\r # Python\rimport numpy as np\rq25, q75 = np.percentile(data, 25), np.percentile(data, 75)\riqr = q75 - q25\r Bước 2: Tính giá trị biên Upper/Lower để xác định outliers\n# thiết lập giới hạn để xác định outliers\r# R\rlimit_iqr = 1.5*iqr\rlower_iqr = q25 - limit_iqr\rupper_iqr = q75 + limit_iqr\r # Python\rlimit_iqr = 1.5*iqr\rlower_iqr, upper_iqr = q25 - limit_iqr, q75 + limit_iqr\r Bước 3: Xác định và loại bỏ outliers dựa trên giá trị biên\n# xác định outliers\r# R\rouliers_index_iqr \u0026lt;- which(data \u0026gt; upper_iqr | data \u0026lt; lower_iqr)\rprint(paste(\u0026quot;Number of outliers:\u0026quot;, length(ouliers_index_iqr)))\r ## [1] \u0026quot;Number of outliers: 36\u0026quot;\r # Python\rouliers_index_iqr = [x for x in data if x \u0026lt; lower_iqr or x \u0026gt; upper_iqr]\rprint('Number of outliers: %d' % len(ouliers_index_iqr))\r ## Number of outliers: 42\r # Loại bỏ outliers\r# R\rdata_new_iqr \u0026lt;- data[-ouliers_index_iqr]\rprint(paste(\u0026quot;Number of Non-outliers:\u0026quot;, length(data_new_iqr)))\r ## [1] \u0026quot;Number of Non-outliers: 4964\u0026quot;\r # Python\rdata_new_iqr = [x for x in data if x \u0026gt;= lower_iqr and x \u0026lt;= upper_iqr]\rprint('Non-outlier observations: %d' % len(data_new_iqr))\r ## Non-outlier observations: 4958\r 3. Xây dựng hàm tự động xác định và loại bỏ outliers\nChúng ta có thể tạo một hàm trên R dựa vào các bước ở trên để tự động xác định và xóa outliers như sau:\n# R\r# Tạo hàm tự động xác định và loại bỏ outliers bằng phương pháp STD\rfind_outliers_std \u0026lt;- function(data) {\r# tính giá trị biên Upper/Lower\rmean_data \u0026lt;- mean(data)\rstd_data \u0026lt;- sd(data)\rlimit = 3*std_data\rlower = mean_data - limit\rupper = mean_data + limit\r# xác định outliers\rouliers_index \u0026lt;- which(data \u0026gt; upper | data \u0026lt; lower)\r# Thông báo thông tin về các outliers đã xóa\rif (length(ouliers_index) \u0026gt; 0 ) {\rmessage(paste(\u0026quot;Number of outliers:\u0026quot;, length(ouliers_index)))\rmessage(paste(\u0026quot;Number of Non-outliers:\u0026quot;, length(data_new_iqr)))\r# return the data with the outliers removed\rreturn(data[-ouliers_index])\r} else {\rmessage(\u0026quot;Not outliers\u0026quot;)\r}\r}\r Tương tự ta có thể tạo hàm xác định outliers bằng phương pháp IQR như sau:\n# Python\r# Tạo hàm tự động xác định và loại bỏ outliers bằng phương pháp IQR\rfind_outliers_iqr \u0026lt;- function(data) {\r# Tính IQR\rq25 \u0026lt;- quantile(data)[2]\rq75 \u0026lt;- quantile(data)[4]\riqr = q75 - q25 # Tính giá trị biên Upper/Lower để xác định outliers\rupper = q75 + iqr * 1.5\rlower = q25 - iqr * 1.5\r# xác định outliers\rouliers_index \u0026lt;- which(data \u0026gt; upper | data \u0026lt; lower)\r# Thông báo thông tin về các outliers đã xóa\rif (length(ouliers_index) \u0026gt; 0 ) {\rmessage(paste(\u0026quot;Number of outliers:\u0026quot;, length(ouliers_index)))\rmessage(paste(\u0026quot;Number of Non-outliers:\u0026quot;, length(data_new_iqr)))\r# return the data with the outliers removed\r# return the data with the outliers removed\rreturn(data[-ouliers_index])\r} else {\rmessage(\u0026quot;Not outliers\u0026quot;)\r}\r}\r Kiểm tra kết quả thực hiện của hai hàm này:\nnew_data_std \u0026lt;- find_outliers_std(data)\r ## Number of outliers: 18\r ## Number of Non-outliers: 4964\r new_data_iqr \u0026lt;- find_outliers_iqr(data)\r ## Number of outliers: 36\r ## Number of Non-outliers: 4964\r Đối với Python thì các bạn có thể tạo hàm tương tự sử dụng các bước trên\n ","date":1597881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597923807,"objectID":"047cb70185a601a2d3767fc8018a0160","permalink":"/post/remove-outliers/","publishdate":"2020-08-20T00:00:00Z","relpermalink":"/post/remove-outliers/","section":"post","summary":"Xác định và loại bỏ ouliers trong dữ liệu là một bước vô cùng quan trọng giúp tăng cao độ chính xác cho các mô hình dự đoán","tags":["Data Science","Machine Learning","Deep Learning"],"title":"Sử dụng thống kê để xác định và loại bỏ dữ liệu ngoại lai cho machine learning trong R và Python","type":"post"},{"authors":[],"categories":["R","Python"],"content":"\rPackage dplyr là một trong những thành phần rất quan trọng trong hệ sinh thái tidyverse của Hadley Wickham - tác giả của các thư viện nổi tiếng trên R như ggplot2, readr, tidyr,…dplyr cung cấp các công cụ biến đổi giúp cho việc thao tác với dữ liệu trên R dễ dàng và thuận tiện hơn bao giờ hết. Kết hợp với toán tử %\u0026gt;%, nhiều người cho rằng dplyr đã tạo ra concept mới cho R.\nĐể thực hiện các công việc liên quan đến data manipulation chúng ta có thể sử dụng các hàm thuộc gói base trong R, tuy nhiêu cú phát của nhiều hàm base là không nhất quán và khó nhớ, đặc biệt là cho một nhóm công việc đặc biệt tốn thời gian như là data manipulation. Gói hay còn thường được gọi là hệ sinh thái tidyverse ra đời nhằm đáp ứng các nhu cầu ngày càng tăng của việc biến đổi và làm sạch dữ liệu (data cleaning), nó bao gồm tập hợp của những gói mạnh nhất chuyên cho công việc làm sạch và biến đổi dữ liệu. Ưu điểm của việc sử dụng tidyverse phải kể đến các lí do sau:\n\rDễ hiểu và có cú pháp nhất quán.\rHợp nhất và hỗ trợ ggplot2 - một gói chuyên cho data visualization.\rCó thể phân tích, quản lí cơ sở dữ liệu ngoài (external databases) mà không cần biết sâu các chi tiết và kiến thức của các ngôn ngữ truy vấn cơ sở dữ liệu (database query languages) khác như SQL.\r\rNạp gói tidyverse vào phiên làm việc trên R để thực hiện công việc data manipulation:\n# R\rlibrary(tidyverse)\rTidyverse là một hộp công cụ lớn chứa nhiều package nhỏ (trong đó có dplyr) bên trong và các mảnh nhỏ này kết nối với nhau một cách tiện ích để vận hành một quy trình phân tích dữ liệu hoàn chỉnh. Hiện nay tidyverse chứa 27 packages bao gồm:\n# R\rtidyverse_packages()\r## [1] \u0026quot;broom\u0026quot; \u0026quot;cli\u0026quot; \u0026quot;crayon\u0026quot; \u0026quot;dbplyr\u0026quot; \u0026quot;dplyr\u0026quot; ## [6] \u0026quot;forcats\u0026quot; \u0026quot;ggplot2\u0026quot; \u0026quot;haven\u0026quot; \u0026quot;hms\u0026quot; \u0026quot;httr\u0026quot; ## [11] \u0026quot;jsonlite\u0026quot; \u0026quot;lubridate\u0026quot; \u0026quot;magrittr\u0026quot; \u0026quot;modelr\u0026quot; \u0026quot;pillar\u0026quot; ## [16] \u0026quot;purrr\u0026quot; \u0026quot;readr\u0026quot; \u0026quot;readxl\u0026quot; \u0026quot;reprex\u0026quot; \u0026quot;rlang\u0026quot; ## [21] \u0026quot;rstudioapi\u0026quot; \u0026quot;rvest\u0026quot; \u0026quot;stringr\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;tidyr\u0026quot; ## [26] \u0026quot;xml2\u0026quot; \u0026quot;tidyverse\u0026quot;\rTrong post này chúng ta hãy cùng so sánh các thao tác data manipulation thường dùng trong data science bằng cách sử dụng R (với dplyr) và sử dụng Python (với pandas):\n\rfilter: Lọc dữ liệu với các điều kiện của biến (thuộc tính)\rselect: Chọn thuộc tính hay là tập hợp các thuộc tính\rarrange: Sắp xếp dữ liệu theo thứ t\rmutate : Tạo thuộc tính mới cho data frameự\rsummarise: Thống kê mô tả theo phân nhóm\r\rNạp thư viện reticulate:\n# R\rlibrary(reticulate)\rreticulate cho phép chúng ta sử dụng kết hợp R và Python trong môi trường R. Cụ thể về cách sử dụng thư viện này để kết hợp R và Python tôi đã giới thiệu ở post trước, các bạn có thể đọc ở đây\nNạp thư viện pandas trong môi trường R:\n# Python\rimport pandas as pd\rChú ý: Những R code chunks tôi sẽ comment R và Python là Python.\nCũng giống như dplyr trong R, về cơ bản Pandas là một thư viện mã nguồn mở, được cộng đồng đánh giá là high-performance, nó hỗ trợ đắc lực trong thao tác với dữ liệu và giúp cho việc xử lý dữ liệu, tính toán sẽ dễ dàng hơn rất nhiều cách truyền thống trong Python.\nLoad dữ liệu để demo:\n# R\r# Nạp R built-in dataset mtcars\rdata(mtcars)\r# Chuyển dữ dữ liệu về dạng tibble(hay còn gọi là data_frame) - đây là 1 # kiểu cấu trúc dữ liệu mới trong R nhằm đáp ứng nhu cầu về tốc độ tính toán\r# khi xử lí dữ liệu có kích thước lớn\rdata_r \u0026lt;- as_tibble(mtcars)\rdata_r\r## # A tibble: 32 x 11\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4\r## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4\r## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1\r## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1\r## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2\r## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1\r## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4\r## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2\r## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2\r## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4\r## # ... with 22 more rows\rSo sánh dplyr và pandas\n1. Filter dữ liệu theo điều kiện của thuộc tính\n# R\rfilter(data_r, cyl \u0026gt;=6 \u0026amp; am == 1)\r## # A tibble: 5 x 11\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4\r## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4\r## 3 15.8 8 351 264 4.22 3.17 14.5 0 1 5 4\r## 4 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6\r## 5 15 8 301 335 3.54 3.57 14.6 0 1 5 8\r# Python\r# chuyển dữ liệu từ R cho Python:\rdata_py = r.data_r\rdata_py[(data_py[\u0026#39;cyl\u0026#39;] \u0026gt;=6) \u0026amp; (data_py[\u0026#39;am\u0026#39;] == 1)]\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## 0 21.0 6.0 160.0 110.0 3.90 2.620 16.46 0.0 1.0 4.0 4.0\r## 1 21.0 6.0 160.0 110.0 3.90 2.875 17.02 0.0 1.0 4.0 4.0\r## 28 15.8 8.0 351.0 264.0 4.22 3.170 14.50 0.0 1.0 5.0 4.0\r## 29 19.7 6.0 145.0 175.0 3.62 2.770 15.50 0.0 1.0 5.0 6.0\r## 30 15.0 8.0 301.0 335.0 3.54 3.570 14.60 0.0 1.0 5.0 8.0\r# R\rdata_r %\u0026gt;% filter(am != 0)\r## # A tibble: 13 x 11\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4\r## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4\r## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1\r## 4 32.4 4 78.7 66 4.08 2.2 19.5 1 1 4 1\r## 5 30.4 4 75.7 52 4.93 1.62 18.5 1 1 4 2\r## 6 33.9 4 71.1 65 4.22 1.84 19.9 1 1 4 1\r## 7 27.3 4 79 66 4.08 1.94 18.9 1 1 4 1\r## 8 26 4 120. 91 4.43 2.14 16.7 0 1 5 2\r## 9 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2\r## 10 15.8 8 351 264 4.22 3.17 14.5 0 1 5 4\r## 11 19.7 6 145 175 3.62 2.77 15.5 0 1 5 6\r## 12 15 8 301 335 3.54 3.57 14.6 0 1 5 8\r## 13 21.4 4 121 109 4.11 2.78 18.6 1 1 4 2\r# Python\rdata_py[data_py[\u0026#39;am\u0026#39;] != 0]\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## 0 21.0 6.0 160.0 110.0 3.90 2.620 16.46 0.0 1.0 4.0 4.0\r## 1 21.0 6.0 160.0 110.0 3.90 2.875 17.02 0.0 1.0 4.0 4.0\r## 2 22.8 4.0 108.0 93.0 3.85 2.320 18.61 1.0 1.0 4.0 1.0\r## 17 32.4 4.0 78.7 66.0 4.08 2.200 19.47 1.0 1.0 4.0 1.0\r## 18 30.4 4.0 75.7 52.0 4.93 1.615 18.52 1.0 1.0 4.0 2.0\r## 19 33.9 4.0 71.1 65.0 4.22 1.835 19.90 1.0 1.0 4.0 1.0\r## 25 27.3 4.0 79.0 66.0 4.08 1.935 18.90 1.0 1.0 4.0 1.0\r## 26 26.0 4.0 120.3 91.0 4.43 2.140 16.70 0.0 1.0 5.0 2.0\r## 27 30.4 4.0 95.1 113.0 3.77 1.513 16.90 1.0 1.0 5.0 2.0\r## 28 15.8 8.0 351.0 264.0 4.22 3.170 14.50 0.0 1.0 5.0 4.0\r## 29 19.7 6.0 145.0 175.0 3.62 2.770 15.50 0.0 1.0 5.0 6.0\r## 30 15.0 8.0 301.0 335.0 3.54 3.570 14.60 0.0 1.0 5.0 8.0\r## 31 21.4 4.0 121.0 109.0 4.11 2.780 18.60 1.0 1.0 4.0 2.0\r# R\rdata_r %\u0026gt;% group_by(cyl) %\u0026gt;% filter(sum(hp) \u0026gt; 2000)\r## # A tibble: 14 x 11\r## # Groups: cyl [1]\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2\r## 2 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4\r## 3 16.4 8 276. 180 3.07 4.07 17.4 0 0 3 3\r## 4 17.3 8 276. 180 3.07 3.73 17.6 0 0 3 3\r## 5 15.2 8 276. 180 3.07 3.78 18 0 0 3 3\r## 6 10.4 8 472 205 2.93 5.25 18.0 0 0 3 4\r## 7 10.4 8 460 215 3 5.42 17.8 0 0 3 4\r## 8 14.7 8 440 230 3.23 5.34 17.4 0 0 3 4\r## 9 15.5 8 318 150 2.76 3.52 16.9 0 0 3 2\r## 10 15.2 8 304 150 3.15 3.44 17.3 0 0 3 2\r## 11 13.3 8 350 245 3.73 3.84 15.4 0 0 3 4\r## 12 19.2 8 400 175 3.08 3.84 17.0 0 0 3 2\r## 13 15.8 8 351 264 4.22 3.17 14.5 0 1 5 4\r## 14 15 8 301 335 3.54 3.57 14.6 0 1 5 8\r# Python\rdata_py.groupby(\u0026#39;cyl\u0026#39;).filter(lambda x: sum(x[\u0026#39;hp\u0026#39;]) \u0026gt; 2000)\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## 4 18.7 8.0 360.0 175.0 3.15 3.440 17.02 0.0 0.0 3.0 2.0\r## 6 14.3 8.0 360.0 245.0 3.21 3.570 15.84 0.0 0.0 3.0 4.0\r## 11 16.4 8.0 275.8 180.0 3.07 4.070 17.40 0.0 0.0 3.0 3.0\r## 12 17.3 8.0 275.8 180.0 3.07 3.730 17.60 0.0 0.0 3.0 3.0\r## 13 15.2 8.0 275.8 180.0 3.07 3.780 18.00 0.0 0.0 3.0 3.0\r## 14 10.4 8.0 472.0 205.0 2.93 5.250 17.98 0.0 0.0 3.0 4.0\r## 15 10.4 8.0 460.0 215.0 3.00 5.424 17.82 0.0 0.0 3.0 4.0\r## 16 14.7 8.0 440.0 230.0 3.23 5.345 17.42 0.0 0.0 3.0 4.0\r## 21 15.5 8.0 318.0 150.0 2.76 3.520 16.87 0.0 0.0 3.0 2.0\r## 22 15.2 8.0 304.0 150.0 3.15 3.435 17.30 0.0 0.0 3.0 2.0\r## 23 13.3 8.0 350.0 245.0 3.73 3.840 15.41 0.0 0.0 3.0 4.0\r## 24 19.2 8.0 400.0 175.0 3.08 3.845 17.05 0.0 0.0 3.0 2.0\r## 28 15.8 8.0 351.0 264.0 4.22 3.170 14.50 0.0 1.0 5.0 4.0\r## 30 15.0 8.0 301.0 335.0 3.54 3.570 14.60 0.0 1.0 5.0 8.0\r2. Select thuộc tính\n# R\rhead(select(data_r, cyl, hp, wt), 5)\r## # A tibble: 5 x 3\r## cyl hp wt\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 110 2.62\r## 2 6 110 2.88\r## 3 4 93 2.32\r## 4 6 110 3.22\r## 5 8 175 3.44\r# Python\rdata_py[[\u0026#39;cyl\u0026#39;, \u0026#39;hp\u0026#39;, \u0026#39;wt\u0026#39;]].head(5)\r## cyl hp wt\r## 0 6.0 110.0 2.620\r## 1 6.0 110.0 2.875\r## 2 4.0 93.0 2.320\r## 3 6.0 110.0 3.215\r## 4 8.0 175.0 3.440\r# R\rhead(select(data_r, -mpg), 5)\r## # A tibble: 5 x 10\r## cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 160 110 3.9 2.62 16.5 0 1 4 4\r## 2 6 160 110 3.9 2.88 17.0 0 1 4 4\r## 3 4 108 93 3.85 2.32 18.6 1 1 4 1\r## 4 6 258 110 3.08 3.22 19.4 1 0 3 1\r## 5 8 360 175 3.15 3.44 17.0 0 0 3 2\r# Python\rdata_py.drop(\u0026#39;mpg\u0026#39;, 1).head(5)\r## cyl disp hp drat wt qsec vs am gear carb\r## 0 6.0 160.0 110.0 3.90 2.620 16.46 0.0 1.0 4.0 4.0\r## 1 6.0 160.0 110.0 3.90 2.875 17.02 0.0 1.0 4.0 4.0\r## 2 4.0 108.0 93.0 3.85 2.320 18.61 1.0 1.0 4.0 1.0\r## 3 6.0 258.0 110.0 3.08 3.215 19.44 1.0 0.0 3.0 1.0\r## 4 8.0 360.0 175.0 3.15 3.440 17.02 0.0 0.0 3.0 2.0\r3.Sắp Xếp dữ liệu theo thứ tự\n# R\r# Sắp xếp dữ liệu theo giá trị tăng dần của cột cyl\rarrange(data_r, cyl)\r## # A tibble: 32 x 11\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1\r## 2 24.4 4 147. 62 3.69 3.19 20 1 0 4 2\r## 3 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2\r## 4 32.4 4 78.7 66 4.08 2.2 19.5 1 1 4 1\r## 5 30.4 4 75.7 52 4.93 1.62 18.5 1 1 4 2\r## 6 33.9 4 71.1 65 4.22 1.84 19.9 1 1 4 1\r## 7 21.5 4 120. 97 3.7 2.46 20.0 1 0 3 1\r## 8 27.3 4 79 66 4.08 1.94 18.9 1 1 4 1\r## 9 26 4 120. 91 4.43 2.14 16.7 0 1 5 2\r## 10 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2\r## # ... with 22 more rows\r# Python\rdata_py.sort_values(\u0026#39;cyl\u0026#39;).head(10)\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## 31 21.4 4.0 121.0 109.0 4.11 2.780 18.60 1.0 1.0 4.0 2.0\r## 2 22.8 4.0 108.0 93.0 3.85 2.320 18.61 1.0 1.0 4.0 1.0\r## 27 30.4 4.0 95.1 113.0 3.77 1.513 16.90 1.0 1.0 5.0 2.0\r## 26 26.0 4.0 120.3 91.0 4.43 2.140 16.70 0.0 1.0 5.0 2.0\r## 25 27.3 4.0 79.0 66.0 4.08 1.935 18.90 1.0 1.0 4.0 1.0\r## 20 21.5 4.0 120.1 97.0 3.70 2.465 20.01 1.0 0.0 3.0 1.0\r## 7 24.4 4.0 146.7 62.0 3.69 3.190 20.00 1.0 0.0 4.0 2.0\r## 8 22.8 4.0 140.8 95.0 3.92 3.150 22.90 1.0 0.0 4.0 2.0\r## 19 33.9 4.0 71.1 65.0 4.22 1.835 19.90 1.0 1.0 4.0 1.0\r## 18 30.4 4.0 75.7 52.0 4.93 1.615 18.52 1.0 1.0 4.0 2.0\r# R\r# Sắp xếp dữ liệu theo giá trị giảm dần của cột cyl\rarrange(data_r, desc(cyl))\r## # A tibble: 32 x 11\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2\r## 2 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4\r## 3 16.4 8 276. 180 3.07 4.07 17.4 0 0 3 3\r## 4 17.3 8 276. 180 3.07 3.73 17.6 0 0 3 3\r## 5 15.2 8 276. 180 3.07 3.78 18 0 0 3 3\r## 6 10.4 8 472 205 2.93 5.25 18.0 0 0 3 4\r## 7 10.4 8 460 215 3 5.42 17.8 0 0 3 4\r## 8 14.7 8 440 230 3.23 5.34 17.4 0 0 3 4\r## 9 15.5 8 318 150 2.76 3.52 16.9 0 0 3 2\r## 10 15.2 8 304 150 3.15 3.44 17.3 0 0 3 2\r## # ... with 22 more rows\r# Python\rdata_py.sort_values(\u0026#39;cyl\u0026#39;, ascending=False).head(10)\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## 16 14.7 8.0 440.0 230.0 3.23 5.345 17.42 0.0 0.0 3.0 4.0\r## 30 15.0 8.0 301.0 335.0 3.54 3.570 14.60 0.0 1.0 5.0 8.0\r## 4 18.7 8.0 360.0 175.0 3.15 3.440 17.02 0.0 0.0 3.0 2.0\r## 28 15.8 8.0 351.0 264.0 4.22 3.170 14.50 0.0 1.0 5.0 4.0\r## 6 14.3 8.0 360.0 245.0 3.21 3.570 15.84 0.0 0.0 3.0 4.0\r## 24 19.2 8.0 400.0 175.0 3.08 3.845 17.05 0.0 0.0 3.0 2.0\r## 23 13.3 8.0 350.0 245.0 3.73 3.840 15.41 0.0 0.0 3.0 4.0\r## 22 15.2 8.0 304.0 150.0 3.15 3.435 17.30 0.0 0.0 3.0 2.0\r## 21 15.5 8.0 318.0 150.0 2.76 3.520 16.87 0.0 0.0 3.0 2.0\r## 11 16.4 8.0 275.8 180.0 3.07 4.070 17.40 0.0 0.0 3.0 3.0\r4. Tạo thuộc tính mới\n# R\r# Tạo và thêm biến mới có tên cyl2\rmutate(data_r, cyl2 = mean(cyl))\r## # A tibble: 32 x 12\r## mpg cyl disp hp drat wt qsec vs am gear carb cyl2\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 6.19\r## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 6.19\r## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 6.19\r## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 6.19\r## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6.19\r## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 6.19\r## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 6.19\r## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 6.19\r## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 6.19\r## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 6.19\r## # ... with 22 more rows\r# Python\rimport numpy as np\rdata_py.assign(cyl2 = lambda x: np.mean(x.cyl)).head(10)\r## mpg cyl disp hp drat wt qsec vs am gear carb cyl2\r## 0 21.0 6.0 160.0 110.0 3.90 2.620 16.46 0.0 1.0 4.0 4.0 6.1875\r## 1 21.0 6.0 160.0 110.0 3.90 2.875 17.02 0.0 1.0 4.0 4.0 6.1875\r## 2 22.8 4.0 108.0 93.0 3.85 2.320 18.61 1.0 1.0 4.0 1.0 6.1875\r## 3 21.4 6.0 258.0 110.0 3.08 3.215 19.44 1.0 0.0 3.0 1.0 6.1875\r## 4 18.7 8.0 360.0 175.0 3.15 3.440 17.02 0.0 0.0 3.0 2.0 6.1875\r## 5 18.1 6.0 225.0 105.0 2.76 3.460 20.22 1.0 0.0 3.0 1.0 6.1875\r## 6 14.3 8.0 360.0 245.0 3.21 3.570 15.84 0.0 0.0 3.0 4.0 6.1875\r## 7 24.4 4.0 146.7 62.0 3.69 3.190 20.00 1.0 0.0 4.0 2.0 6.1875\r## 8 22.8 4.0 140.8 95.0 3.92 3.150 22.90 1.0 0.0 4.0 2.0 6.1875\r## 9 19.2 6.0 167.6 123.0 3.92 3.440 18.30 1.0 0.0 4.0 4.0 6.1875\r5. Thống kê mô tả theo phân nhóm\n# R\rdata_r %\u0026gt;% group_by(cyl, am) %\u0026gt;% summarise(mean_cyl = mean(cyl),\rsum_cyl = sum(cyl),\rcount_cyl = n())\r## `summarise()` regrouping output by \u0026#39;cyl\u0026#39; (override with `.groups` argument)\r## # A tibble: 6 x 5\r## # Groups: cyl [3]\r## cyl am mean_cyl sum_cyl count_cyl\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 4 0 4 12 3\r## 2 4 1 4 32 8\r## 3 6 0 6 24 4\r## 4 6 1 6 18 3\r## 5 8 0 8 96 12\r## 6 8 1 8 16 2\r# Python\rdata_py.groupby([\u0026#39;cyl\u0026#39;, \u0026#39;am\u0026#39;])[\u0026#39;cyl\u0026#39;].agg([\u0026#39;mean\u0026#39;, \u0026#39;sum\u0026#39;, \u0026#39;count\u0026#39;])\r## mean sum count\r## cyl am ## 4.0 0.0 4.0 12.0 3\r## 1.0 4.0 32.0 8\r## 6.0 0.0 6.0 24.0 4\r## 1.0 6.0 18.0 3\r## 8.0 0.0 8.0 96.0 12\r## 1.0 8.0 16.0 2\r6. Tóm tắt\nDưới đây là tóm tắt so sánh một số thao tác với dữ liệu hay dùng nhất trong Python và R\nQuerying, Filtering, Sampling\n\r\rR\rPython\r\r\r\rdim(df)\rdf.shape\r\rhead(df)\rdf.head()\r\rslice(df, 1:10)\rdf.iloc[:9]\r\rfilter(df, col1 == 1, col2 ==1)\rdf.query(‘col1 == 1 \u0026amp; col2 == 1’)\r\rdf[df\\(col1 == 1 \u0026amp; df\\)col2 == 1]\rdf[(df.col1 == 1) \u0026amp; (df.col2 == 1)]\r\rselect(df, col1, col2)\rdf[[‘col1’, ‘col2’]]\r\rselect(df, col1:col3)\rdf.loc[:, ‘col1’:‘col3’]\r\rselect(df, -(col1:col3))\rdf.drop(cols_to_drop, axis=1)\r\rdistinct(select(df, col1))\rdf[[‘col1’]].drop_duplicates()\r\rdistinct(select(df, col1, col2))\rdf[[‘col1’, ‘col2’]].drop_duplicates()\r\rsample_n(df, 10)\rdf.sample(n=10)\r\rsample_frac(df, 0.01)\rdf.sample(frac=0.01)\r\r\r\rSorting\n\r\rR\rPython\r\r\r\rarrange(df, col1, col2)\rdf.sort_values([‘col1’, ‘col2’])\r\rarrange(df, desc(col1))\rdf.sort_values(‘col1’, ascending=False)\r\r\r\rGrouping and Summarizing\n\r\r\r\rR\rPython\r\r\r\rsummary(df)\rdf.describe()\r\rgdf \u0026lt;- group_by(df, col1)\rgdf = df.groupby(‘col1’)\r\rsummarise(gdf, avg=mean(col1, na.rm=TRUE))\rdf.groupby(‘col1’).agg({‘col1’: ‘mean’})\r\rsummarise(gdf, total=sum(col1))\rdf.groupby(‘col1’).sum()\r\r\r\r","date":1597795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597805730,"objectID":"22a172dc616f8f6ef601139e610dbd9d","permalink":"/post/data-manipulation/","publishdate":"2020-08-19T00:00:00Z","relpermalink":"/post/data-manipulation/","section":"post","summary":"So sánh các thao tác biến đổi dữ liệu với thư viện `dplyr` trong R và `pandas` trong Python","tags":["Data Wrangling","Data Manipulation"],"title":"So sánh dplyr và pandas cho data manipulation","type":"post"},{"authors":["Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"41e1599cafdb3013cb1e512a9badf1fa","permalink":"/publication/journal-article6/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/publication/journal-article6/","section":"publication","summary":"Scientific and technical progress has contributed to a rapid increase in the complexity of systems and their functions, which is especially characteristic of various fields of modern industry. Here, the cost of failure of equipment can be very high and sometimes lead to invaluable losses associated with the loss of life. Maintenance of such systems requires high material costs but still does not exclude the possibility of failures. This indicates that the problem of ensuring the reliability of complex multiobject systems is still far from being solved. In this regard, the task of ensuring reliable operation of systems while minimizing the cost of their maintenance and maintenance is now in the first place. The solution of this problem is impossible without the development and implementation of intelligent systems that perform the functions of predictive analytics and predictive maintenance. This article proposes a hybrid neural network model for predicting failures of complex multi-object systems based on the classification approach, aimed at improving the operational reliability of equipment at a minimal cost. The results of computational experiments confirming the high efficiency of the proposed solution are presented.","tags":["Source Themes"],"title":"А classification approach based on a combination of deep neural networks for predicting failures of complex multi-object systems","type":"publication"},{"authors":["Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1583625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583625600,"objectID":"d5d20be84aa461b6d2f1df8ef2a1336c","permalink":"/publication/journal-article2/","publishdate":"2020-03-08T00:00:00Z","relpermalink":"/publication/journal-article2/","section":"publication","summary":"The research of the problem of automatic high-frequency time series forecasting (without expert) is devoted. The efficiency of high-frequency time series forecasting using different statistical and machine learning modelsis investigated. Theclassical statistical forecasting methods are compared with neural network models based on 1000 synthetic sets of high-frequency data. The neural network models give better prediction results, however, it takes more time to compute compared to statistical approaches.","tags":["Source Themes"],"title":"STATISTICAL AND MACHINE LEARNING HIGH-FREQUENCY TIME SERIES FORECASTING METHODS IN AUTOMATIC MODE","type":"publication"},{"authors":["Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"841d5e04e34ea395095fdcdac436706f","permalink":"/publication/journal-article7/","publishdate":"2020-03-11T00:00:00Z","relpermalink":"/publication/journal-article7/","section":"publication","summary":"The paper proposes a hybrid neural network model with two outputs based on convolutional Neural networks (CNN) and long short-term memory networks (LSTM) for predicting failures of complex multi-component systems. CNN networks are used to extract spatial properties from multidimensional sensor data, and LSTM networks are used for temporal modeling of long-term dependencies. The first output of the proposed model is a classifier that allows you to predict whether the system may fail in the next n-steps of time in the future, in other words, it is an identifier of the stage of degradation of the equipment. The second output is a regressor that allows us to predict the number of the remaining useful life (RUL) of the equipment at each time step.  The results of computational experiments confirming the high efficiency of the proposed solution are presented.","tags":["Source Themes"],"title":"Failure prediction of complex multiple-component systems based on a combination of neural networks':' Ways to improve the accuracy of forecasting","type":"publication"},{"authors":["Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1576800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576800000,"objectID":"e742187ea36c14a88e5db8c319a32de4","permalink":"/publication/journal-article1/","publishdate":"2020-06-04T00:00:00Z","relpermalink":"/publication/journal-article1/","section":"publication","summary":"Properly formed strategy of maintenance of equipment plays a critical role in modern economic condi-tions characterized by crisis phenomena and high levels of competition. Recently, as part of the implementation of the concept of Industry 4.0 in the field of maintenance of complex multi-object systems, the most promising approaches are based on the use of advanced meth-ods for analyzing large data based on innovative artificial intelligence technologies. It is mainly about the concept of predictive maintenance (PdM), namely the creation of predictive models to prevent equipment failures. This maintenance strategy allows to move from time-based maintenance to condi-tion based maintenance, taking into account the prediction of changes in system states in order to achieve their maximum performance at minimal cost. Therefore, this paper discusses the key elements for implementing the PdM strategy. As a result, an architecture for predictive maintenance of complex multi-object systems in the con-cept of Industry 4.0 is proposed. The proposed system includes three modules':' an offline-analysis module for accumulated data, an online-analysis module for streaming data, and a decision support module. The main functions of the first two modules are early detection and prediction of equipment failure based on machine learning methods. Based on the information received from the online analysis module, the decision support module generates optimal decisions when choosing a strategy for influ-encing the equipment, if necessary. Such solutions maintain an optimal balance between the cost of performing technological impacts and the magnitude of potential damages and risks from equipment failure.","tags":["Source Themes"],"title":"Architecture of predictive maintenance system of complex multi-object systems in Industry 4.0 concept","type":"publication"},{"authors":["Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1576800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576800000,"objectID":"b057bc5f579dd349167e06419d10ca54","permalink":"/publication/conference-paper3/","publishdate":"2019-12-20T00:00:00Z","relpermalink":"/publication/conference-paper3/","section":"publication","summary":"In the current manufacturing world, maintenance has a critical role to play in improving companies' competitiveness. Among the available maintenance strategies, predictive maintenance seems to be the most promising because failures are predicted and a timely reaction is possible. Therefore, in this paper, we propose the PdM package to build predictive maintenance models for proactive decision support based on machine learning algorithms. The proposed package implemented as a package for R and it provides several major functionalities that attempt to streamline the process for creating predictive maintenance models. The PdM package also provides interactive graphical user interface (web-application), that enables the user to conduct all steps of the predictive maintenance building workflows from his browser without using code. The main aim of the proposed tool is to allow for exploring different machine learning algorithms when solving the problem of remaining useful life prediction for complex multi-component systems. For illustrations, the proposed tool is applied to the Turbofan Engine Degradation Simulation data set FD001 from NASA for the estimation of the turbofan engine remaining useful life (RUL).","tags":["Source Themes"],"title":"PdM':' A predictive maintenance modeling tool implemented as R-package and web-application","type":"publication"},{"authors":["Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1573171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573171200,"objectID":"a7f8f27c37e8cee6f5500e48965d1bc6","permalink":"/publication/journal-article4/","publishdate":"2019-11-08T00:00:00Z","relpermalink":"/publication/journal-article4/","section":"publication","summary":"At present, deep neural networks are becoming one of the most popular approaches in solving various practical problems from a wide variety of fields, such as image and speech recognition, natural language processing, computer vision, medical informatics, etc. The article considers the possibility of using deep neural networks in the implementation of proactive maintenance strategy – predictive maintenance (PdM). Various methods of constructing predictive models for PdM are considered. Currently, the data-driven approaches using deep neural networks for constructing predictive models for PdM is the most promising method. One of the reasons for the successful application of deep neural networks is that the networks automatically select important features from the data needed to solve the problem. The most commonly used neural networks for PdM are considered':' Long short-term memory (LSTM), convolutional neural networks (CNN), and autoencoders. An overview of powerful frameworks for the design and training of neural networks is given.","tags":["Source Themes"],"title":"DEEP NEURAL NETWORKS FOR PREDICTIVE MAINTENANCE","type":"publication"},{"authors":["Cuong Sai","Maxim Shcherbakov","Phu Tran"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1566259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566259200,"objectID":"d6e6088f81e1b2f425d8eb99ae5a9e79","permalink":"/publication/conference-paper2/","publishdate":"2019-08-20T00:00:00Z","relpermalink":"/publication/conference-paper2/","section":"publication","summary":"Supporting the operation of the equipment at the operational stage with minimal costs is an urgent task for various industries. In the modern manufacturing industry machines and systems become more advanced and complicated, traditional approaches (corrective and preventive maintenance) to maintenance of complex systems lose their effectiveness. The latest trends of maintenance lean towards condition-based maintenance (CBM) techniques. This paper describes the framework to build predictive maintenance models for proactive decision support based on machine learning and deep learning techniques. The proposed framework implemented as a package for R, and it provides several features that allow to create and evaluate predictive maintenance models. All features of the framework can be attributed to one of the following groups':' data validation and preparation, data exploration and visualization, feature engineering, data preprocessing, model creating and evaluation. The use case provided in the paper highlights the benefits of the framework toward proactive decision support for the estimation of the turbofan engine remaining useful life (RUL).","tags":["Source Themes"],"title":"Data-Driven Framework for Predictive Maintenance in Industry 4.0 Concept","type":"publication"},{"authors":null,"categories":null,"content":"Sử dụng R Shiny để phát triển ứng dụng web tương tác (interactive web app), cho phép tracking và visualization luồng dữ liệu đa cảm biến, phát hiện bất thường, dự đoán hỏng hóc và tuổi thọ của máy móc trong thời gian thực.\n  ","date":1553040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553040000,"objectID":"9e2cbd9d06bb4932f3829f56f8ee93b2","permalink":"/project/pdm-project/","publishdate":"2019-03-20T00:00:00Z","relpermalink":"/project/pdm-project/","section":"project","summary":"Sử dụng `R Shiny` phát triển ứng dụng web tương tác để tracking luồng dữ liệu đa cảm biến trong thời gian thực.","tags":["Machine Learning","Data Science","IoT","Predictive Maintenance"],"title":"The PdM Project","type":"project"},{"authors":["Andrey Davydenko","Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1552953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552953600,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2019-03-19T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"we propose flexible yet simple data schemas allowing the storage and exchange of actuals, forecasts, and additional variables of interest.","tags":["Source Themes"],"title":"Data Formats and Visual Tools for Forecast Evaluation","type":"publication"},{"authors":["Cuong Sai","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1552003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552003200,"objectID":"114eb2cc5a80e7a63a7ad85ac647049c","permalink":"/publication/journal-article3/","publishdate":"2019-03-08T00:00:00Z","relpermalink":"/publication/journal-article3/","section":"publication","summary":"Supporting the operation of the equipment at the operational stage with minimal costs is an urgent task for various industries. Classical approaches to maintenance of systems lose their effectiveness in modern conditions. The article describes a data-driven method of proactive maintenance of equipment based on predicting the remaining useful life (RUL). The main purpose of this research is to develop a method of predicting the RUL aimed at minimizing operating costs in the maintenance of equipment. Contributions of the paper are':' i) a new architecture of real-time predictive maintenance system; ii) the study of the effectiveness of different approaches (both deep neural networks and typical algorithms of machine learning) in RUL predicting; iii) a new hybrid CNN+LSTM model based on the combination of convolutional neural networks (CNN) and long short-term memory networks (LSTM), superior analogs in solving the problem of predicting the RUL using Turbofan Engine Degradation Simulation Data Set from NASA.","tags":["Source Themes"],"title":"A Data-driven method for remaining useful life prediction of complex multiple-component systems","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Cuong Sai","Andrey Davydenko","Maxim Shcherbakov"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1543017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543017600,"objectID":"d74b8c67a747ee925e2d38307b763f0b","permalink":"/publication/conference-paper1/","publishdate":"2018-11-24T00:00:00Z","relpermalink":"/publication/conference-paper1/","section":"publication","summary":"Evaluation of forecasting performance using real-world data is inevitably connected with the question of how to store actuals and forecasts in a convenient way. The issue gets complicated when it comes to working with rolling-origin out-of-sample forecasts calculated for many time series. This setup can be met in both research tasks (such as forecasting competitions or when some new method is proposed) and in practical settings. When designing data schemas for forecasting it is important to provide access to the information needed for exploratory time series analysis and accuracy evaluation. We found that existing approaches to store forecasting data often cannot be applied efficiently as they are either not flexible enough or they require too much resources to implement and maintain the data storage. Here we propose a flexible yet simple way of keeping forecasting data allowing the storage and exchange of actuals, forecasts, and other relevant information. We also present an R package that helps perform exploratory data analysis and accuracy evaluation based on the data schemas proposed.","tags":["Source Themes"],"title":"Data Schemas for Forecasting (with Examples in R)","type":"publication"},{"authors":["Maxim Shcherbakov","Alexey Golubev","Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"54db35530773a630679e7ecc9f8e1acc","permalink":"/publication/conference-paper5/","publishdate":"2018-09-14T00:00:00Z","relpermalink":"/publication/conference-paper5/","section":"publication","summary":"Increasing the complexity of distributed multi-objects system-of-interest leads to the need to revise the existing mechanisms and methods by which decision-making mechanisms for securing these systems are provided. One of the foremost areas of improving support processes is the use of intelligent systems. The use of these systems allows moving from planned provision to functioning according to the state, taking into account the prediction of changes in system states. In the framework of this study, the problem of synthesis of predictive management solutions is considered to provide effective support for the operation of complex multi-object systems in the concept of proactive computing.","tags":["Source Themes"],"title":"Generating Proactive Decisions Using a Method Based on LSTM and Classification","type":"publication"},{"authors":["Phu Tran","Maxim Shcherbakov","Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1535068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535068800,"objectID":"69ea2d521456c13b2db5388ff88ee9cc","permalink":"/publication/conference-paper4/","publishdate":"2018-08-24T00:00:00Z","relpermalink":"/publication/conference-paper4/","section":"publication","summary":"Augmented reality application requires both type of data analysis':' image recognition or segmentation and appropriate data extracting related to semantic of image. Obtained information matched and provided for end user. The latency is a crucial point, so the fast approaches are mandatory to use. We suggest an approach for combining multiple sources analyses and positioning for augmented reality (AR)-based application. Distinctive features of the method are (1) the use of the coordinates of the observer and camera and the moving object to clarify the position; (2) identification of the object using image recognition and (3) processing of log data obtained from vehicles. The proposed method in this study can be applied in augmented reality-based decision support system which requires obtain and proceed data from multiple data sources. The proposed method was applied to the traffic analysis task based on video streaming and log data analysis where location of observer is the similar to location of camera.","tags":["Source Themes"],"title":"On-the-Fly Multiple Sources Data Analysis in AR-Based Decision Support Systems","type":"publication"},{"authors":["Phu Tran","Maxim Shcherbakov","Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1528070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528070400,"objectID":"f9f4acc6fc4e0e832c487756c429e751","permalink":"/publication/journal-article5/","publishdate":"2018-12-08T00:00:00Z","relpermalink":"/publication/journal-article5/","section":"publication","summary":"The problem of storage and processing of heterogeneous data (both structured and non-structured) from various data sources is an important issue when implementing proactive support systems for a life cycle operation stage of complex multi-object distributed systems. The data are heterogeneous, so it is required to store data descriptions (metadata) for subsequent automatic processing. In order to solve the problem of heterogeneous data-efficient storage, an architecture called data lake is used. It implements mechanisms for data batch processing and real-time data processing.  The task of improving methods for effective access to heterogeneous data includes the following subtasks':' development of SQL query grammar for heterogeneous data; building a parser for recognizing queries according to new grammar; development of modules for processing heterogeneous data according to a query; development of recommendations (methods) for applying the developed modules in proactive decision support systems. The proposed grammar is based on the DML extension of the SQL language, in particular the SELECT statement extension. There is the generated parser using the ANTLR 3.0 library for processing the generated queries according to new grammar. Due to generation, there are some created classes in JAVA with their objects used for parsing queries. The generated parser and processing modules for heterogeneous data have become a basis for the new software. After testing the software that implements the proposed grammar in the proactive decision support system, the authors have analyzed the time of execution of unified queries with different volumes of heterogeneous data. The main result of the grammar application is the reduction in the heterogeneous data processing time within a single query.","tags":["Source Themes"],"title":"Grammar for queries for heterogeneous data storage in proactive systems","type":"publication"},{"authors":[],"categories":["Python","R"],"content":"\rMột câu hỏi thường hay đặt ra bởi nhiều nhà khoa học dữ liệu, đặc biệt là những người mới trong lĩnh vực này đó là: Python hay R tốt hơn cho khoa học dữ liệu? Thực ra còn nhiều ngôn ngữ khác cho khoa học dữ liệu như là: Java, C, Scala, Matlab, Julia,…nhưng Python và R đang dẫn đầu trong phần lớn thế giới khoa học dữ liệu.\nR vốn được thiết kế bởi các nhà thống kê và đã trở thành lựa chọn hàng đầu cho những ai bước vào lĩnh vực khoa học dữ liệu. Một trong những thế mạnh chính của R là số lượng khổng lồ các packages chuyên dụng và cộng đồng support vô cùng lớn. Tuy nhiên, độ phổ biến của nó gần đây đã giảm đi một chút.\nTrong khi đó, Python là ngôn ngữ đa mục đích có thể sử dụng trong nhiều lĩnh vực khác nhau từ lập trình ứng dụng web đến viết game. Có thể thấy là hiện nay Python đang thách thức ngôi vị lâu đầu của R như là một ngôn ngữ chung cho các nhà khoa học dữ liệu.\nTuy nhiên các doanh nghiệp sẽ không ưu tiên ngôn ngữ nào khi xem xét các kỹ năng của một nhà khoa học dữ liệu, vì vậy các bạn phải biết cách linh hoạt trong việc sử dụng các ngôn ngữ này làm công cụ cho khoa học dữ liệu làm sao hiệu quả nhất.\nCả R và Python đều có sử dụng để giải quyết các bài toán tương tự nhau trong khoa học dữ liệu. Cả hai đều có những packages hoặc libraries để giải quyết những vấn đề trong phân tích dữ liệu như tiền xủ lý, ứng dụng machine learning cho dữ liệu. Cả hai đều là lựa chọn tốt cho nghiên cứu khả lặp - một kỹ năng đòi hỏi trong rất nhiều lĩnh vực khác nhau. Tuy nhiên hai ngôn ngữ này lại có những tính năng tuyệt vời và điểm mạnh riêng vì vậy chúng ta nên kết hợp chúng để tận dụng những lợi thế của nhau, chỗ nào tiện và mạnh cái gì thì dùng cái đó. Hình dưới đây mô tả tóm tắt những điểm nổi trội của hai ngôn ngữ này:\nỞ post này tôi sẽ giới thiệu với các bạn cách kết hợp hai ngôn ngữ này trong môi trường R bằng cách sử dụng package reticulate.\nTrước hết để tiến hành làm việc với Python và R chúng ta cần cài đặt chúng:\n\rPython: Nếu bạn muốn đơn giản hóa việc tạo môi trường khi thực hiện các dự án bằng Python, đặc biệt liên quan đến các lĩnh vực khoa học dữ liệu, AI, Data, … Anaconda sẽ là lựa chọn bạn nên cân nhắc. Bạn có thể tải và cài Anaconda tại đây\rR: Để làm việc với R các bạn cần tải R và Rstudio. Tải và cài phiên bản R mới nhất tại đây. Tải và cài đặt RStudion tại đây\r\rĐể kết hợp R và Python trong môi trường R chúng ta cần cài R package reticulate bằng câu lệnh sau:\ninstall.packages(\u0026quot;reticulate\u0026quot;)\rTuy nhiên cách cài đặt như các bạn vừa thực hiện ở trên là không khuyến khích. Nguyên nhân là một\rsố gói của R để sử dụng được còn phụ thuộc vào một hoặc một số gói khác. Do vậy an toàn nhất là cài\rđặt với lựa chọn dependencies = TRUE với hàm ý rằng chúng ta sẽ cài đặt luôn tất cả các gói phụ\rthuộc:\ninstall.packages(\u0026quot;reticulate\u0026quot;, dependencies = TRU)\rPackage reticulate gồm các công cụ hỗ trợ tương tác giữa R và Python. Cụ thể package này giải quyết được những việc sau:\n\rGọi Python từ R bằng những phương pháp khác nhau: từ R Markdown, từ mã nguồn Python, import các Python modules, và sử dụng tương tác trực tiếp trên R session.\rChuyển đổi các đối tượng R và Python với nhau (ví dụ giữa các data frames R và Pandas, giữa các R matrices và NumPy arrays)\rLiên kết linh hoạt giữa các phiên bản Python khác nhau bao gồm cả virtual environments và Conda environments.\r\rKêt hợp R và Python\nĐể có thể dùng Python trong môi trường R chúng ta cần nạp thư thư viện reticulate theo câu lệnh sau:\nlibrary(reticulate)\rĐể kiểm tra môi trường Python có trong máy, các bạn dùng câu lệnh sau:\nconda_list()\r## name\r## 1 Anaconda3\r## 2 TF\r## 3 rstudio\r## 4 r-reticulate\r## 5 r-tf-gpu\r## python\r## 1 C:\\\\Users\\\\svcuo\\\\Anaconda3\\\\python.exe\r## 2 C:\\\\Users\\\\svcuo\\\\Anaconda3\\\\envs\\\\TF\\\\python.exe\r## 3 C:\\\\Users\\\\svcuo\\\\Anaconda3\\\\envs\\\\rstudio\\\\python.exe\r## 4 C:\\\\Users\\\\svcuo\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-reticulate\\\\python.exe\r## 5 C:\\\\Users\\\\svcuo\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-tf-gpu\\\\python.exe\rDùng hàm use_python() cho phép bạn chỉ định rõ phiên bản Python cần thiết:\nuse_python(\u0026quot;C:/Users/svcuo/Anaconda3/python.exe\u0026quot;)\rSử dụng hàm use_virtualenv() và use_condaenv() cho phép bạn chỉ định phiên bản Python của bạn làm việc trong môi trường virtual hay là Conda:\nuse_virtualenv(\u0026quot;myenv\u0026quot;)\rTa có thể cài đặt bất kỳ python library nào trong R bằng cách dùng hàm py_install() như sau:\npy_install(\u0026quot;pandas\u0026quot;)\rCác cách sử dụng kết hợp Python và R trong môi trường R\n1. Import các python modules\nSử dụng hàm import() cho phép bạn có thể gọi mọi python library và sử dụng trong R:\nos \u0026lt;- import(\u0026quot;os\u0026quot;)\ros$listdir(\u0026quot;.\u0026quot;)\r [1] \u0026quot;.RData\u0026quot; \u0026quot;.Rhistory\u0026quot; \u0026quot;.Rproj.user\u0026quot; \u0026quot;assets\u0026quot; \u0026quot;config\u0026quot; [6] \u0026quot;config.toml\u0026quot; \u0026quot;content\u0026quot; \u0026quot;index.Rmd\u0026quot; \u0026quot;myblog.Rproj\u0026quot; \u0026quot;public\u0026quot; [11] \u0026quot;resources\u0026quot; \u0026quot;static\u0026quot; \u0026quot;themes\u0026quot; \r2. Sourcing Python scripts\nBạn có thể nạp bất kỳ một Python scripts nào vào R sử dụng hàm source_python(). Ví dụ bạn có một Python script tên là flights.py có dạng:\nimport pandas as pd\rdef read_flights(file):\rflights = pd.read_csv(file)\rflights = flights[flights[\u0026#39;dest\u0026#39;] == \u0026quot;ORD\u0026quot;]\rflights = flights[[\u0026#39;carrier\u0026#39;, \u0026#39;dep_delay\u0026#39;, \u0026#39;arr_delay\u0026#39;]]\rflights = flights.dropna()\rreturn flights\rVà các bạn có thể nạp và thực thi nó trên R như sau:\nsource_python(\u0026quot;RPython/flights.py\u0026quot;)\rflights \u0026lt;- read_flights(\u0026quot;flights.csv\u0026quot;)\rlibrary(ggplot2)\rggplot(flights, aes(carrier, arr_delay)) + geom_point() + geom_jitter()\r3. Python REPL\nHàm repl_python() cho phép các bạn có thể làm việc trực tiếp với Python trên R session. Các đối tượng được tạo ra từ Python REPL có thể được truy cập từ R bằng cách sử dụng py$object. Ví dụ:\nDùng lệnh exit để thoát python REPL\nChú ý: Trong Python code có thể sử dụng mọi đối tượng được tạo ra từ R bằng các sử dụng đối tượng r.object (ví dụ r.flights).\n4. Sử dụng python trong R markdown\nPackage reticulate hỗ trợ Python engine cho R Markdown với những thuộc tính sau:\n\rChạy Python code chunks tích hợp trong R session( cho phép trao đổi các đối tượng với nhau)\rHiển thị Python output, bao gồm cả output từ matplotlib\rTruy cập tới các đối tượng được tạo ra từ Python chunks và ngược lại từ R chunks (sử dụng py$object và r.object)\r\rChúng ta hãy cùng xem ví dụ kết hợp R và Python đơn giản sau:\nNạp dữ liệu vào Python:\n# Python\rimport pandas as pd\rdataset_url = \u0026#39;https://raw.githubusercontent.com/forvis/MLdata/master/pima-indians-diabetes.data.csv\u0026#39;\rnames = [\u0026#39;preg\u0026#39;, \u0026#39;plas\u0026#39;, \u0026#39;pres\u0026#39;, \u0026#39;skin\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;mass\u0026#39;, \u0026#39;pedi\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;class\u0026#39;]\rdata = pd.read_csv(dataset_url, names = names)\rprint(data.head())\r## preg plas pres skin test mass pedi age class\r## 0 6 148 72 35 0 33.6 0.627 50 1\r## 1 1 85 66 29 0 26.6 0.351 31 0\r## 2 8 183 64 0 0 23.3 0.672 32 1\r## 3 1 89 66 23 94 28.1 0.167 21 0\r## 4 0 137 40 35 168 43.1 2.288 33 1\rSau đó chuyển dữ liệu và thao tác với dữ liệu bằng R:\n# R\rtable(py$data$preg)\r## ## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 ## 111 135 103 75 68 57 50 45 38 28 24 11 9 10 2 1 1\rThực hiện data visualization trong R vơi base function boxplot():\n# R\rx \u0026lt;- py$data[,1:8]\ry \u0026lt;- py$data[,9]\r# Boxplot for first 4 attribute\rnames = c(\u0026#39;preg\u0026#39;, \u0026#39;plas\u0026#39;, \u0026#39;pres\u0026#39;, \u0026#39;skin\u0026#39;)\rpar(mfrow=c(1,4))\rfor(i in 1:4) {\rboxplot(x[,i], main=names[i])\r}\rThực hiện data visualization trong R với ggplot2:\n# R\rlibrary(ggplot2)\rpy$data$class \u0026lt;- factor(py$data$class)\rggplot(data = py$data,aes(x = preg)) +\rgeom_histogram(binwidth = 0.5,aes(fill = class),position = \u0026quot;dodge\u0026quot;) +\rggtitle(\u0026quot;Pregnancies Data Distribution\u0026quot;) + ylab(\u0026quot;class Counts\u0026quot;) +\rtheme_gray() +\rtheme_update(plot.title = element_text(hjust = 0.5))\rTham khảo:\nR Interface to Python\n","date":1524009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524026438,"objectID":"8b532e9b010b776126df2c8a8a80eb60","permalink":"/post/s-d-ng-k-t-h-p-r-va-python-trong-data-science/","publishdate":"2018-04-18T00:00:00Z","relpermalink":"/post/s-d-ng-k-t-h-p-r-va-python-trong-data-science/","section":"post","summary":"Một câu hỏi thường hay đặt ra bởi nhiều nhà khoa học dữ liệu, đặc biệt là những người mới trong lĩnh vực này đó là: Python hay R tốt hơn cho khoa học dữ liệu?","tags":["Data Science","R Markdown"],"title":"Sử dụng kết hợp R và Python trong data science","type":"post"},{"authors":null,"categories":null,"content":"The project’s aim is to develop effective means for forecast evaluation and visualization. Based on our experience of implementing data science solutions for forecasting, we outline a typical forecast evaluation setup. Especially, we focus on handling rolling-origin forecasts made across many series over multiple horizons when many alternative methods are used.\nWe developed a general framework containing of the following components:\n  Forecast data formats - simple yet flexible enough data structures designed to store data needed for rolling-origin forecast evaluation.\n  Exploratory analysis tools needed to make sure that data is of acceptable quality.\n  Performance measurement tools - techniques and graphs for accuracy and comparison.\n  Generally, the workflow we propose involves the following steps: 1) data preparation, 2) exploratory analysis, 3) performance measurement using appropriate metrics. The project\u0026rsquo;s website describes this process in more detail here\n","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"c86b850a8f4cee747d8fa2642220cb13","permalink":"/project/forvision-project/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/project/forvision-project/","section":"project","summary":"The project’s aim is to develop effective means for forecast evaluation and visualization.","tags":["Forecast evaluation","Data Science"],"title":"The Forvision Project","type":"project"},{"authors":["Maxim Shcherbakov","Phu Tran","Anh Nguyen","Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1505865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505865600,"objectID":"a11a2e235a9cef92c03f40b678568fbb","permalink":"/publication/paten3/","publishdate":"2017-09-20T00:00:00Z","relpermalink":"/publication/paten3/","section":"publication","summary":"Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.","tags":["Source Themes"],"title":"Распределенная система слияния и преобработки разнородных данных с разных источников","type":"publication"},{"authors":["Cuong Sai"],"categories":[],"content":"Khoa học dữ liệu (data science, DS) là một trong những ngành đang có nhu cầu tuyển dụng cao ở thời điểm hiện tại cũng như trong tương lai xa. Nhà khoa học dữ liệu (data scientist) được Harvard Business Review đánh giá là “công việc hấp dẫn nhất thế kỷ 21”.\nVậy DS là gì? Trong thực tế một định nghĩa chính xác về DS không tồn tại, đây là một khái niệm liên ngành và rất rộng. DS là một lĩnh vực về các quá trình và các hệ thống trích rút tri thức hoặc hiểu biết dữ liệu ở các dạng khác nhau với biên độ rộng của các ngành như: Toán học, khoa học thống kê, khoa học thông tin, khoa học máy tính và lĩnh vực chuyên môn cụ thể. Bao gồm xử lý tín hiệu, lý thuyết xác suất thống kê, học máy, khai phá dữ liệu, cơ sở dữ liệu, kỹ thuật thông tin, nhận dạng mẫu, trực quan dữ liệu, các phân tích dự đoán, lý thuyết quyết định, kho dữ liệu, nén dữ liệu, lập trình máy tính, trí tuệ nhân tạo và siêu máy tính. Có thể nói \u0026ldquo;Một nhà khoa học dữ liệu là người giỏi hơn về thống kê so với những kỹ sư phát triển phần mềm và giỏi hơn về lập trình so với những nhà thống kê học.\u0026rdquo; Về bản chất có thể hiểu khoa học dữ liệu là ngành giúp chúng ta tạo ra giá trị từ dữ liệu với hai nhiệm vụ chính:\n Thu thập và xử lý dữ liệu để tìm ra những insights có giá trị. Giải thích, trình bày những insights đó cho các bên có liên quan để chuyển hóa isights thành hành động.  Xuât phát từ tên gọi DS ta có hai thành phần cấu thành là data và science:\n  Data: Là thành phần thứ nhất của cụm từ data science, thiếu nó thì tất cả các quá trình tiếp theo đều không thể thực hiện. Sau khi đã có đầy đủ dữ liệu cần thiết, trước khi sử dụng, công việc đầu tiên bạn cần làm là làm sạch và biến đổi dữ liệu - bước quan trọng nhất trong phân tích dữ liệu, nó chiếm đến 80% tổng số thời gian thực hiện phân tích.\n  Science: Chúng ta đã có dữ liệu vậy bây giờ làm gì với chúng? Đó là phân tích, trích rút các quy luật có ích và làm sao có để có thể sử dụng chúng một cách hiệu quả. Ở đây các lĩnh vực sẽ giúp chúng ta như là thống kê, máy học, học sâu, tối ưu. Máy học giúp chúng ta tìm ra các quy luật trong dữ liệu để có thể dự báo thông tin cần thiết với đối tượng mói sau đó.\n  DS và trí tuệ nhân tạo (Artificial Intelligence, AI) thường bị đánh tráo khái niệm. Để phân biệt chính xác hai khái niệm về công nghệ này, hãy cùng tham khảo bài viết nhé!\n Phân biệt các khái niệm DS, AI, machine learning và deep learning\nThực ra các lĩnh vực này đều có sự liên quan, chồng chéo đan xen với nhau. AI là một trong các công cụ cho DS. DS sử dụng AI trong các hoạt động của mình, tuy nhiên chắc chắn nó không bao hàm AI. Hay nói cách khác trong DS có sự đóng góp bởi một số khía cạnh của AI nhưng lại không phản ánh tất cả về AI. Hình dưới đây mô tả mối quan hệ của các khái niệm này:\nsource: Gmggroup.org\nAI là một ngành khoa học được sinh ra với mục đích làm cho máy tính có được trí thông minh. Machine learning là một phương tiện được kỳ vọng sẽ giúp con người đạt được mục tiêu đích đó. Cụ thể, AI - là mục tiêu đặt ra, còn machine learning - là phương tiện, công cụ để đạt được mục tiêu đó. Còn deep learning là một kỹ thuật bổ sung cho machine learning. Đây là một thuật toán dự trên ý tưởng từ bộ não của con người.\n Ngôn ngữ lập trình cho khoa học dữ liệu\nChọn ngôn ngữ nào để bắt đầu sự nghiệp Khoa học dữ liệu sẽ có nhiều thử thách đối với bạn. Có một số ngôn ngữ phục vụ cho ngành này phải kể đến những ngôn ngữ lập trình sau:\nR: là một trong những platforms mạnh cho học máy, thống kê và phân tích dữ liệu được phát triển và sử dụng bởi các nhà khoa học thống kê và phân tích dữ liệu hàng đầu trên thế giới. Nếu bạn muốn đi sâu vào phân tích dữ liệu và thống kê, thì R là ngôn ngữ dành cho bạn.Ngôn ngữ này đã có sự tăng trưởng vượt bậc khi phân tích dữ liệu và khoa học dữ liệu trở nên phổ biến hơn trong những năm gần đây. Hàng năm, số lượng người dùng R tăng hơn 40% và ngày càng có nhiều cơ quan và tổ chức sử dụng R trong hoạt động phân tích thường nhật Tuy nhiên trong thời rian gần đây độ phổ biến của nó đã giảm đi một chút.\nPython: là một ngôn ngữ lập trình đa mục đích, khá mạnh và bao gồm các công cụ có thể ứng dụng vào các môi trường yêu cầu hình tượng hoá mà có thể xuất hiện trên các trang web hoặc trên điện thoại. Python cung cấp hỗ trợ cho một số lượng lớn các thư viện học sâu như Pandas, Matplotlib, Tensorflow, Keras, scikit-learn, v.v. Để bắt đầu với ngành Data Science. Python là một trong những ngôn ngữ lập trình lý tưởng và nó cũng dễ đọc hơn R.\nJava: Ngôn ngữ lập trình Java gần đây được xếp hạng một trong những ngôn ngữ được yêu thích và đa năng nhất để viết, dựa vào bản khảo sát từ WP Engine. Nó cũng là một ngôn ngữ lập trình đa mục đích, được thiết kế riêng để càng ít phụ thuộc vào việc thực thi càng tốt. Nó có thể được sử dụng để xây dựng mọi thứ, đặc biệt là các nền tảng có thể mở rộng, nền tảng đa luồng (multithread) và có nền tảng người dùng mạnh mẽ. ác framework như Apache Spark, Hadoop và Hive ngày càng phổ biến trong môi trường thương mại, làm cho Java trở thành một trong những ngôn ngữ được các nhà khoa học dữ liệu yêu cầu. Kiến thức về Java sẽ tạo điều kiện cho bạn điều chỉnh và duy trì các nền tảng dữ liệu lớn như Hadoop được viết bằng cùng một ngôn ngữ.\nNgoài ra còn một số ngôn ngữ khác như là Julia, Scala, \u0026hellip;\n Tham khảo:\n The differences between Data Science, Artificial Intelligence, Machine Learning, and Deep Learning\n","date":1502928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502928000,"objectID":"01f1e18610006559efc38cc712f55236","permalink":"/post/2020-08-17-khai-niem-co-ban/","publishdate":"2017-08-17T00:00:00Z","relpermalink":"/post/2020-08-17-khai-niem-co-ban/","section":"post","summary":"Khái niệm Data Science, Artificial Intelligence, Machine Learning, Deep Learning","tags":[],"title":"Mở đầu về khoa học dữ liệu. Các khái niệm cơ bản","type":"post"},{"authors":["Александр Любовец","Леонид Чувычкин","Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1468886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468886400,"objectID":"64da77a1e6de2b610dc0f3733f7dde71","permalink":"/publication/paten2/","publishdate":"2016-07-19T00:00:00Z","relpermalink":"/publication/paten2/","section":"publication","summary":"Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.","tags":["Source Themes"],"title":"Программа расчета асимптотических оценок показателей эффективностей системы эксплуатационного котроля объекта авиационной техники","type":"publication"},{"authors":["Александр Любовец","Леонид Чувычкин","Cuong Sai"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1466467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466467200,"objectID":"74bfb6f256e3f061325850759cb1d1a1","permalink":"/publication/paten1/","publishdate":"2016-06-21T00:00:00Z","relpermalink":"/publication/paten1/","section":"publication","summary":"Программа предназначена для расчета статистических оценок показателей эффективности системы эксплуатационного контроля объекта авиационной техники по данным эксплуатации с учетом инструментальной достоверности и полноты контроля':' средних удельных потерь времени использования воздушного судна; средней удельной стоимости контроля. Программа осуществляет выборку случайных значений параметров системы эксплуатационного контроля по заданному закону распределения для расчета и построения графических зависимостей математических ожиданий и средних квадратических отклонений показателей эффективности от изменения параметров при заданном количестве выборок статистических параметров. Программа состоит из модулей, содержащих различные математические описания для расчета статистических оценок эффективности типовых вариантов системы эксплуатационного контроля объекта авиационной техники':' периодический наземный контроль бортовыми средствами контроля; периодический контроль наземными средствами перед полетом.","tags":["Source Themes"],"title":"Программа расчета статистических оценок показателей эффективностей системы эксплуатационного котроля объекта авиационной техники","type":"publication"},{"authors":null,"categories":["R"],"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","Plot","Regression"],"title":"Mở đầu với R Markdown","type":"post"}]